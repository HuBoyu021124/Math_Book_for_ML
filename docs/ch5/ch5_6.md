## 5.6 反向传播与自动微分

在许多机器学习的应用中，我们通过计算学习目标关于模型参数的梯度，然后执行梯度下降（见 7.1 节）找好的模型参数。对于给定的目标函数，我们可以利用微积分的链式法则得到其对模型参数的梯度（见 5.2.2 节）。我们在 5.3 节已经尝试对平方损失结果关于线性回归模型参数求梯度。

考虑下面的函数：

$$

f(x) = \sqrt{ x^{2} + \exp(x^{2}) } + \cos \Big[ x^{2} + \exp(x^{2}) \Big]. \tag{5.109}

$$

由链式法则，并注意到微分的线性性，我们可以得到：

$$

\begin{align}

\displaystyle \frac{ \mathrm{d}f }{ \mathrm{d}x } &= \frac{2x+2x\exp\{x^{2}\}}{2\sqrt{ x^{2} + \exp\{x^{2}\} }} - \sin \Big( x^{2} + \exp\{ x^{2} \} \Big) \Big( 2x + 2x \exp\{ x^{2} \} \Big) \\

&= 2x \left[ \frac{1}{2\sqrt{ x^{2} + \exp\{ x^{2} \} }} - \sin \Big( x^{2} + \exp\{ x^{2} \} \Big) \right] \Big(1 + \exp\{ x^{2} \}\Big). 

\end{align}

$$

$$

\tag{5.110}

$$

像这样显式求解得到这样冗长的导数表达往往不切实际。在实践中这意味着若不小心处理，梯度的实现可能比计算函数值要昂贵得多，这增加了不必要的开销。对于神经网络模型，反向传播算法(Kelley, 1960; Bryson, 1961; Dreyfus, 1962; Rumelhart et al., 1986)是一种计算误差对模型参数梯度的有效方法。

### 5.6.1 深度神经网络中的梯度

深度学习领域将链式法则的功用发挥到了极致，输入 $\boldsymbol{x}$ 通过多层复合的函数得到函数值 $\boldsymbol{y}$ ：

$$

\boldsymbol{y} = (f_{K} \circ f_{K-1} \circ \cdots \circ f_{1})(\boldsymbol{x}) = f_{K}\Big\{ f_{K-1}\big[\cdots (f_{1}(\boldsymbol{x})\cdots )\big] \Big\} , \tag{5.111}

$$

其中，$\boldsymbol{x}$ 是输入（如图像），$\boldsymbol{y}$ 是观测值（如类标签），每个函数 $f_{i}, i = 1, \dots, K$，有各自的参数。

![800](attachments/Pasted%20image%2020250131211557.png)

<center>图 5.8 多层神经网络的前向传播</center>

在一般的多层神经网络中，第 $i$ 层中有函数  $f_{i}(\boldsymbol{x}_{i-1}) = \sigma(\boldsymbol{A}_{i-1}\boldsymbol{x}_{i-1} + \boldsymbol{b}_{i-1})$ 。其中 $x_{i-1}$ 是 $i=1$ 层的输出和一个激活函数 $\sigma$，例如 sigmoid 函数 $\displaystyle \frac{1}{1-e^{-x}}$，$\tanh$ 或修正线性单元（rectified linear unit, ReLU）。为了训练这样的模型，我们需要一个损失函数 $L$，对其值求关于所有模型参数 $\boldsymbol{A}_j, \boldsymbol{b}_{j}, j=1, \dots, K$ 的梯度。这同时要求我们求其对模型中各层的输入的梯度。例如，如果我们有输入x和观测值y和一个网络结构，则它被定义为

*<!-- TODO【以下为机器翻译结果，需要进行后期修正】 -->*

$$

\begin{align}

\boldsymbol{f}_{0} &:= \boldsymbol{x} \tag{5.112}\\

\boldsymbol{f}_{i} &:= \sigma_{i} \Big( \boldsymbol{A}_{i-1}\boldsymbol{f}_{i-1} + \boldsymbol{b}_{i-1} \Big), \quad  i=1, \dots, K, \tag{5.113} 

\end{align}

$$

参见图5.8的可视化，我们可能有兴趣找到Aj，bj为j = 0，……，K−1，这样的平方损失

$$

L(\boldsymbol{\theta}) = \Big\| \boldsymbol{y} - \boldsymbol{f}_{K}\big( \boldsymbol{\theta, x} \big)  \Big\|^{2} \tag{5.114}

$$

被最小化，其中θ = {A0，b0，……，AK−1，bK−1}。为了得到参数相对于参数集θ的梯度，我们需要L对每一层j=的参数θj = {Aj，bj}的偏导数0，…，K−1。链式规则允许我们确定偏导数为

$$

\begin{align}

\displaystyle \frac{ \partial L }{ \partial \boldsymbol{\theta}_{K-1} } &= \displaystyle \frac{ \partial L }{ \partial \boldsymbol{f}_{K} } {\color{blue} \displaystyle \frac{ \partial \boldsymbol{f}_{K} }{ \partial \boldsymbol{\theta}_{K-1} } } \tag{5.115}\\

\displaystyle \frac{ \partial L }{ \partial \boldsymbol{\theta}_{K-2} }  &= \displaystyle \frac{ \partial L }{ \partial \boldsymbol{f}_{K} } \boxed{ {\color{orange} \displaystyle \frac{ \partial \boldsymbol{f}_{K} }{ \partial \boldsymbol{f}_{K-1} }  } {\color{blue} \displaystyle \frac{ \partial \boldsymbol{f}_{K-1} }{ \partial \boldsymbol{\theta}_{K-2} }  }}\tag{5.116}\\  

\displaystyle \frac{ \partial L }{ \partial \boldsymbol{\theta}_{K-3} } &= \displaystyle \frac{ \partial L }{ \partial \boldsymbol{f}_{K} } {\color{orange} \displaystyle \frac{ \partial \boldsymbol{f}_{K} }{ \partial \boldsymbol{f}_{K-1} }  } \boxed{ {\color{orange} \displaystyle \frac{ \partial \boldsymbol{f}_{K-1} }{ \partial \boldsymbol{f}_{K-2} }  } {\color{blue} \displaystyle \frac{ \partial \boldsymbol{f}_{K-2} }{ \partial \boldsymbol{\theta}_{K-3} }  } } \tag{5.117}\\

\displaystyle \frac{ \partial L }{ \partial \boldsymbol{\theta}_{i} }  &= \displaystyle \frac{ \partial L }{ \partial \boldsymbol{f}_{K} } {\color{orange} \displaystyle \frac{ \partial \boldsymbol{f}_{K} }{ \partial \boldsymbol{f}_{K-1} } \cdots } \boxed{ {\color{orange} \displaystyle \frac{ \partial \boldsymbol{f}_{i+2} }{ \partial \boldsymbol{f}_{i+1} }  } {\color{blue} \displaystyle \frac{ \partial \boldsymbol{f}_{i+1} }{ \partial \boldsymbol{\theta}_{i} }  }  } \tag{5.118}

\end{align}

$$

橙色的项是一个层的输出相对于其输入的偏导数，而蓝色的项是一个层的输出相对于其参数的偏导数。假设，我们已经计算出了偏导数∂L/∂θi+1，那么大部分的计算都可以被重用来计算∂L/∂θi。我们所提供的附加条款需要计算的是用方框表示。图5.9显示了梯度通过网络向后传递。

### 5.6.2 自动微分

结果表明，反向传播是数值分析中一般采用的自动微分技术的一种特殊情况。我们可以把自动差分看作是一组技术，通过处理中间变量和应用链规则，用数值（与符号化相反）来评估一个函数的精确（直到机器精度）梯度。自动微分应用一系列初等算术运算，如加法、乘法和初等函数，如sin、cos、exp、log。通过将链式规则应用于这些操作，可以自动计算出相当复杂的函数的梯度。自动微分适用于一般的计算机程序，并具有正向和反向模式。Baydin等人（2018）对机器学习中的自动分化进行了很好的概述。图5.10显示了一个简单的图，通过一些中间变量a，b表示从输入x到输出y的数据流。如果我们要计算导数dy/dx，我们将应用链规则并得到

$$

\displaystyle \frac{ \mathrm{d}y }{ \mathrm{d}x }  = \displaystyle \frac{ \mathrm{d}y }{ \mathrm{d}b } \displaystyle \frac{ \mathrm{d}b }{ \mathrm{d}a } \displaystyle \frac{ \mathrm{d}a }{ \mathrm{d}x } . \tag{5.119}

$$

直观地说，正模和反模在多重的顺序上不同-在一般情况下，我们使用雅可比矩阵，它可以是向量、矩阵或张量。阳离子。由于矩阵乘法的结合性，我们可以从中进行选择

$$

\begin{align}

\displaystyle \frac{ \mathrm{d}y }{ \mathrm{d}x } &= \left( \displaystyle \frac{ \mathrm{d}y }{ \mathrm{d}b } \displaystyle \frac{ \mathrm{d}b }{ \mathrm{d}a }  \right) \displaystyle \frac{ \mathrm{d}a }{ \mathrm{d}x } , \tag{5.120}\\

\displaystyle \frac{ \mathrm{d}y }{ \mathrm{d}x } &= \displaystyle \frac{ \mathrm{d}y }{ \mathrm{d}b } \left( \displaystyle \frac{ \mathrm{d}b }{ \mathrm{d}a } \displaystyle \frac{ \mathrm{d}a }{ \mathrm{d}x }  \right). \tag{5.121}

\end{align}

$$

方程（5.120）将是反向模式，因为梯度通过图向后传播，即反向传播到数据流。公式（5.121）是正向模式，其中梯度与数据从左到右流动。

下面，我们将重点关注反向模式的自动微分，即反向传播。在神经网络中，输入的维数通常比标签的维数高得多，反向模式在计算上比正向模式低得多。让我们从一个有益的例子开始

$$

f(x) = \sqrt{ x^{2} + \exp\{ x^{2} \} } + \cos \Big( x^{2} + \exp\{ x^{2} \} \Big) \tag{5.122}

$$

从（5.109）开始。如果我们要在计算机上实现一个函数f，我们将能够通过使用中间变量来节省一些计算：

$$

\begin{align}

a &= x^{2}, \tag{5.123}\\

b &= \exp\{ a \}, \tag{5.124}\\

c &= a + b, \tag{5.125}\\

d &= \sqrt{ c }, \tag{5.126}\\

e &= \cos(c), \tag{5.127}\\

f &= d + e. \tag{5.128}\\

\end{align}

$$

这是在应用链规则时发生的相同的思维过程。请注意，前面一组方程所需要的操作比（5.109）中定义的函数f (x)的直接实现要少。图5.11中对应的计算图显示了获得函数值f所需的数据流和计算量。包含中间变量的方程组可以被认为是一个计算图，这是一种广泛应用于神经网络软件库实现中的表示方法。通过回顾初等函数的导数的定义，我们可以直接计算中间变量与其相应输入的导数。我们获得以下信息：

$$

\begin{align}

\displaystyle \frac{ \partial a }{ \partial x } &= 2x \tag{5.129}\\

\displaystyle \frac{ \partial b }{ \partial a } &= \exp\{ a \}\tag{5.130}\\

\displaystyle \frac{ \partial c }{ \partial a } &= 1 = \displaystyle \frac{ \partial c }{ \partial b } \tag{5.131}\\

\displaystyle \frac{ \partial d }{ \partial c } &= \frac{1}{2\sqrt{ c }}\tag{5.132}\\

\displaystyle \frac{ \partial e }{ \partial c } &= -\sin(c)\tag{5.133}\\

\displaystyle \frac{ \partial f }{ \partial d } &= 1 = \displaystyle \frac{ \partial f }{ \partial e }. \tag{5.134}

\end{align}

$$

通过查看图5.11中的计算图，我们可以通过从输出中向后工作来计算∂f/∂x并得到

$$

\begin{align}

\displaystyle \frac{ \partial f }{ \partial c } &= \displaystyle \frac{ \partial f }{ \partial d } \displaystyle \frac{ \partial d }{ \partial c }  + \displaystyle \frac{ \partial f }{ \partial e } \displaystyle \frac{ \partial e }{ \partial c } \tag{5.135}\\

\displaystyle \frac{ \partial f }{ \partial b } &= \displaystyle \frac{ \partial f }{ \partial c } \displaystyle \frac{ \partial c }{ \partial b } \tag{5.136}\\

\displaystyle \frac{ \partial f }{ \partial a } &= \displaystyle \frac{ \partial f }{ \partial b } \displaystyle \frac{ \partial b }{ \partial a } + \displaystyle \frac{ \partial f }{ \partial c } \displaystyle \frac{ \partial c }{ \partial a } \tag{5.137}\\

\displaystyle \frac{ \partial f }{ \partial x } &= \displaystyle \frac{ \partial f }{ \partial a } \displaystyle \frac{ \partial a }{ \partial x }. \tag{5.138}\\

\end{align}

$$

注意，我们隐式地应用了链规则来获得∂f/∂x。用初等函数导数的结果，得到

$$

\begin{align}

\displaystyle \frac{ \partial f }{ \partial c } &= 1 \cdot \frac{1}{2\sqrt{ c }} + 1 \cdot \big[ -\sin(c) \big] \tag{5.139}\\

\displaystyle \frac{ \partial f }{ \partial b } &= \displaystyle \frac{ \partial f }{ \partial c } \cdot 1\tag{5.140}\\

\displaystyle \frac{ \partial f }{ \partial a } &= \displaystyle \frac{ \partial f }{ \partial b } \exp\{ a \} + \displaystyle \frac{ \partial f }{ \partial c } \cdot 1 \tag{5.141}\\

\displaystyle \frac{ \partial f }{ \partial x } &= \displaystyle \frac{ \partial f }{ \partial a }  \cdot 2x. \tag{5.142}

\end{align}

$$

通过将上面的每个导数作为一个变量，我们观察到计算导数所需的计算与函数本身的计算的复杂性相似。这是非常违反直觉的，因为导数∂f∂x（5.110）的数学表达式比（5.109）中的函数f (x)的数学表达式要复杂得多。

自动区分是实施例5.14的形式化。设x1，……，xd是函数的输入变量，xd+1，……，xd−1是中间变量，xD是输出变量。则计算图可以表示为：

$$

\text{For }i = d+1, \dots, D:\quad x_{i} = g_{i}\Big[x_{\text{Pa}(x_{i})}\Big], \tag{5.143}

$$

其中，gi（·）是基本函数，xPa（xi）是图中变量xi的父节点。给定一个以这种方式定义的函数，我们可以使用链式规则来逐步计算该函数的导数。回想一下，根据定义，f = xD，因此

$$

\displaystyle \frac{ \partial f }{ \partial x_{D} } =1. \tag{5.144}

$$

对于其他变量xi，我们应用链式规则

$$

\displaystyle \frac{ \partial f }{ \partial x_{i} } = \sum\limits_{x_{j}: x_{i} \in \text{Pa}(x_{j})} \displaystyle \frac{ \partial f }{ \partial x_{j} } \displaystyle \frac{ \partial x_{j} }{ \partial x_{i} } = \sum\limits_{x_{j}: x_{i} \in \text{Pa}(x_{j})} \displaystyle \frac{ \partial f }{ \partial x_{j} } \displaystyle \frac{ \partial g_{j} }{ \partial x_{i} } ,\tag{5.145}  

$$

其中，Pa（xj）是计算图中xj的父节点的集合。方程（5.143）是一个函数的正向传播，而（5.145）是该梯度通过计算图的反向传播。对于神经网络训练，我们反向传播关于标签的预测误差。当我们有一个可以表示为计算图的函数时，其中初等函数是可微的。事实上，这个函数甚至可能不是一个数学函数，而是一个计算机程序。然而，并不是所有的计算机程序都可以自动微分，例如，如果我们不能找到微分的初等函数。编程结构，如循环和if语句，也需要更多的小心。