# 第五章 向量微积分
许多机器学习算法都在优化一个目标函数，即相对于一组模型参数进行优化，这些参数控制着模型解释数据的好坏。如何寻找好的参数可被表述为一个优化问题（见 8.2 节和 8.3 节）。优化的例子包括：
1. 线性回归（见第9章），我们研究曲线拟合问题，并优化线性权重参数以最大化可能性；
2. 神经网络自编码器用于降维和数据压缩，其中参数是每层的权重和偏差，我们通过反复应用链式法则来最小化重建误差；
3.  Gauss 混合模型（见第11章）用于建模数据分布，我们优化每个混合组件的位置和形状参数，以最大化模型的可能性。
图5.1展示了我们通常使用利用梯度信息（第7.1节）的优化算法来解决这些问题。图5.2概述了本章概念之间以及它们与书中其他章节的联系。

本证的核心概念是函数。一个函数 $f$ 是一个数学对象，它将两个数学对象进行联系。本书中涉及的数学对象即为模型输入 $\boldsymbol{x} \in \mathbb{R}^{D}$ 以及拟合目标（函数值）$f(\boldsymbol{x})$，若无额外说明，默认拟合目标都是实数。这里 $\mathbb{R}^{D}$ 称为 $f$ 的**定义域（domain）**，而相对应的函数值 $f(\boldsymbol{x})$ 所在的集合被称为 $f$ 的**像集（image）或陪域（codomain）**。

![](.../attachments/Pasted%20image%2020240825122538.png)
<center>图 5.1 向量微积分在 (a) 回归问题（曲线拟合）和 (b) 分布密度估计（建模数据分布） <br>中有重要应用。</center>

![](.../attachments/Pasted%20image%2020240825122711.png)
<center>图 5.2 本章的概念地图及与其他章节的联系</center>

2.7.3 节中有对线性函数更为细致的讨论，但一般而言，我们将函数写为下面的形式
$$
\begin{align}
f : \mathbb{R}^D &\to \mathbb{R}\tag{5.1a}\\
\boldsymbol{x} &\mapsto f(\boldsymbol{x}) \tag{5.1b}
\end{align}
$$
其中 $(5.1a)$ 说的是 $f$ 是一个由 $\mathbb{R}^{D}$ 至 $\mathbb{R}$ 的映射，而 $(5.2b)$ 指的是 $f$ 将每一个输入 $\boldsymbol{x}$ 对应于唯一的函数值 $f(\boldsymbol{x})$。

> **示例 5.1**
> 请回忆在 3.2 节中我们谈到点积是一种特殊地内积。沿用之前的记号，函数 $f(\boldsymbol{x}) = \boldsymbol{x}^{\top}\boldsymbol{x}, \boldsymbol{x} \in \mathbb{R}^{2}$ 相当于
> $$\begin{align}f: \mathbb{R}^{2} & \rightarrow \mathbb{R}\tag{5.2a}\\\boldsymbol{x} &\mapsto x_{1}^{2} + x_{2}^{2}. \tag{5.2b}\end{align}$$

本章将介绍如何计算函数的梯度——这在机器学习中如何充分利用*学习*非常重要，因为梯度指向函数值提升的最陡峭方向。所以向量微积分是机器学习中所需的基础数学工具。我们在全书中都默认函数是可微的，但若具备一些尚未提及的额外定义，很多机器学习方法可被扩展至**次梯度（sub-differentials，当函数连续但在某些点不可微时）**。我们将在第七章探讨带有条件限制的此类函数。



