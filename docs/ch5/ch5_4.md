## 5.4 矩阵的梯度

接下来我们将会看见需要求矩阵对向量（或其他矩阵）的梯度的情形。它们的结果是一个多维度的*张量（tensor）*，我们可以将其看做装有偏导数的多维数组。例如，如果我们计算一个 $m \times n$ 形状的矩阵 $\boldsymbol{A}$ 对 $p \times q$ 形状的矩阵 $\boldsymbol{B}$ 的梯度，结果Jacobian 矩阵的形状将是 $(m \times n) \times (p \times q)$，即一个四维张量 $\boldsymbol{J}$，它的每个分量可以写为 $\boldsymbol{J}_{i,j,k,l} = \displaystyle \frac{ \partial \boldsymbol{A}_{i,j} }{ \partial \boldsymbol{B}_{k, l} }$。

由于矩阵代表着线性变换。我们可以用这样的事实构造形状为 $m \times n$ 矩阵空间 $\mathbb{R}^{m \times n}$ 到 $mn$ 长度的线性空间 $\mathbb{R}^{mn}$ 的线性空间同构（可逆的线性映射）。这样一来我们就可以调整矩阵 $\boldsymbol{A}$ 和 $\boldsymbol{B}$ 的形状，使其分别变成长度为 $mn$ 和长度为 $pq$ 的向量。因此对这样的向量求梯度就得到形状为 $mn \times pq$ 的Jacobi矩阵。图 5.7 画出了上面两种方法的示意图。实际操作中，将矩阵压扁成向量然后继续处理Jacobi矩阵的方法较受欢迎，因为这样一来链式法则（5.48）就变成简单的矩阵乘法；而如果处理的是Jacobi张量，我们就得对于二者相乘时求和的维度倍加小心。

![](../attachments/attachments/Pasted%20image%2020250106171552.png)

> **示例 5.12（向量对矩阵的梯度）**
> 考虑下面的例子：$$\boldsymbol{f} = \boldsymbol{A}\boldsymbol{x}, \quad \boldsymbol{f} \in \mathbb{R}^{M}, \quad \boldsymbol{A} \in \mathbb{R}^{M \times N}, \quad \boldsymbol{x} \in \mathbb{R}^{N},\tag{5.85}$$求梯度 $\displaystyle \frac{ \mathrm{d}\boldsymbol{f} }{ \mathrm{d}\boldsymbol{A} }$。
> 首先确定梯度的维数：$$\displaystyle \frac{ \mathrm{d}\boldsymbol{f} }{ \mathrm{d}\boldsymbol{A} } \in \mathbb{R}^{M \times (M \times N)}. \tag{5.86}$$按照定义，梯度里面装着一族偏导的结果：$$\displaystyle \frac{ \mathrm{d}\boldsymbol{f} }{ \mathrm{d}\boldsymbol{A} } = \begin{bmatrix}\displaystyle \frac{ \partial f_{i} }{ \partial \boldsymbol{A} } \\ \vdots  \\ \displaystyle \frac{ \partial f_{M} }{ \partial \boldsymbol{A} } \end{bmatrix}, \quad \displaystyle \frac{ \partial f_{i} }{ \partial \boldsymbol{A} } \in \mathbb{R}^{1 \times (M \times N)}. \tag{5.87}$$接下来我们求每一项的值。我们首先根据矩阵乘法分别展开每个结果分量 $f_{i}$：$$f_{i} = \sum\limits_{j=1}^{N} A_{i,j}x_{j}, \quad  i= 1, \dots, M, \tag{5.88}$$然后得到 $f_{i}$ 对矩阵中每一份量的偏导数$$\displaystyle \frac{ \partial f_{i} }{ \partial A_{j,q} } = x_{q}. \tag{5.89}$$将它们一行一行的组合起来，并注意一下结果的形状，我们就得到了 $f_{i}$ 对矩阵 $\boldsymbol{A}$ 中各行的偏导：$$\begin{align}\displaystyle \frac{ \partial f_{i} }{ \partial A_{i,:}} &= \boldsymbol{x}^{\top} \in \mathbb{R}^{1 \times 1 \times N}, \tag{5.90}\\\displaystyle \frac{ \partial f_{i} }{ \partial A_{k\neq i, :} } &= \boldsymbol{0}^{\top} \in \mathbb{R}^{1 \times 1 \times N}, \tag{5.91}\end{align}$$由于 $f_{i}$ 是实值函数，矩阵 $\boldsymbol{A}$ 的每一行形状为 $1×N$，我们得到的 $f_{i}$ 关于矩阵每一行的偏导数张量的形状就是 $1×1×N$。最后我们将（5.91）堆叠起来，就得到所求的梯度（5.87）中的每一项：$$\displaystyle \frac{ \partial f_{i} }{ \partial \boldsymbol{A} } = \begin{bmatrix}\boldsymbol{0}^{\top} \\ \vdots \\ \boldsymbol{0}^{\top} \\ \boldsymbol{x}^{\top} \\ \boldsymbol{0}^{\top} \\ \vdots \\\boldsymbol{0}^{\top}\end{bmatrix} \in \mathbb{R}^{1 \times (M \times N)}. \tag{5.92}$$

> **示例 5.13（矩阵对矩阵的梯度）**
> 给定矩阵 $\boldsymbol{R} \in \mathbb{R}^{M \times N}$，和矩阵值函数 $\boldsymbol{f}: \mathbb{R}^{M \times N} \rightarrow \mathbb{R}^{N \times N}$:$$\boldsymbol{f}(\boldsymbol{R}) = \boldsymbol{R}^{\top}\boldsymbol{R} =: \boldsymbol{K} \in \mathbb{R}^{N \times N}, \tag{5.93}$$求梯度 $\displaystyle \frac{ \mathrm{d}\boldsymbol{K} }{ \mathrm{d}\boldsymbol{R} }$。
> 这个问题有些困难。我们先写下已知信息：梯度的维数$$\displaystyle \frac{ \mathrm{d}\boldsymbol{K} }{ \mathrm{d}\boldsymbol{R} } \in \mathbb{R}^{(N \times N) \times (M \times N)}, \tag{5.94} $$毫无疑问这是个张量。我们进一步写出 $\boldsymbol{K}$ 中每个元素对矩阵 $\boldsymbol{R}$ 的梯度维数：$$\displaystyle \frac{ \mathrm{d}K_{p,q} }{ \mathrm{d}\boldsymbol{R} } \in \mathbb{R}^{1 \times M \times N}, p,q = 1, \dots, N \tag{5.95}$$其中 $\boldsymbol{K}_{p,q}$ 是 $\boldsymbol{K} = \boldsymbol{f} (\boldsymbol{R})$ 中处于第 $p$ 行，第 $q$ 列的元素。用 $\boldsymbol{r}_{i}$ 表示 $\boldsymbol{R}$ 的第 $i$ 列，则 $\boldsymbol{K}$ 中的每个元素可以写成 $\boldsymbol{R}$ 中两列的点积，即$$K_{p,q} = \boldsymbol{r}_{p}^{\top}\boldsymbol{r}_{q} =\sum\limits_{m=1}^{M} R_{m,p}R_{m,q}. \tag{5.96}$$接着我们计算偏导数$$\displaystyle \frac{ \partial K_{p,q} }{ \partial R_{i,j} } = \sum\limits_{m=1}^{M} \displaystyle \frac{ \partial   }{ \partial R_{i,j} } R_{m,p}R_{m,q} = \partial_{p,q,i,j},\tag{5.97}$$其中$$\partial_{p,q,i,j} = \begin{cases}R_{i, q}, & j = p, p \neq q\\R_{i, p}, & j = q, p \neq q\\2R_{i,q}, & j=p, p=q\\0, & \text{其他情形}\end{cases}\quad .\tag{5.98}$$从（9.94）我们知道目标梯度的形状是 $(N \times N) \times (M \times N)$，它的每个分量的值由（5.98）给出，其中 $p,q,j = 1, \dots, N$，$i = 1, \dots, M$。
