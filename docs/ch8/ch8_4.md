## 8.4 概率建模与推断

在机器学习中，我们经常关注数据的解释和分析，例如对未来事件的预测和决策制定。为了使这项任务更易于处理，我们通常会构建模型来描述生成观测数据的生成过程。

例如，我们可以用两个步骤来描述抛硬币实验的结果（正面或反面）。首先，我们定义一个参数$\mu$，它作为伯努利分布（第6章）的参数，描述了出现“正面”的概率；其次，我们可以从伯努利分布$p(x\mid\mu)=\text{Ber}(\mu)$中抽取一个结果$x\in\{\text{head}, \text{tail}\}$。参数$\mu$产生了特定的数据集$\chi$，并且取决于所使用的硬币。由于$\mu$是未知的，且永远无法直接观测到，因此我们需要机制来根据抛硬币实验的观察结果来学习关于$\mu$的信息。接下来，我们将讨论如何使用概率建模来实现这一目的。

### 8.4.1 概率模型

概率模型将实验中的不确定部分表示为概率分布。使用概率模型的好处在于，它们通过概率论提供了一套统一且一致的工具集，这些工具集包括随机变量（第6章），用于建模、推断、预测和模型选择。

在概率建模中，观测变量$x$和隐藏参数$\theta$的联合分布$p(\boldsymbol x,\boldsymbol{\theta})$至关重要：它包含了以下信息：

- 先验和似然（乘积规则，第6.3节）。
- 边缘似然$p(x)$，在模型选择（第8.6节）中扮演重要角色，可以通过联合分布并积分掉参数来计算（求和规则，第6.3节）。
- 后验分布，可以通过将联合分布除以边缘似然来获得。

只有联合分布具有这样的性质。因此，概率模型是由其所有随机变量的联合分布来指定的。

### 8.4.2 贝叶斯推断

机器学习中的一个关键任务是利用模型和数据，在给定观测变量$x$的情况下，揭示模型隐藏变量$\theta$的值。在第8.3.1节中，我们已经讨论了使用最大似然估计或最大后验估计来估计模型参数$\theta$的两种方法。在这两种情况下，我们都获得了$\theta$的一个最佳单一值，因此参数估计的关键算法问题是解决一个优化问题。一旦这些点估计$\theta^*$已知，我们就使用它们来进行预测。更具体地说，预测分布将是$p(\boldsymbol{x}\mid\boldsymbol{\theta}^*)$，其中我们在似然函数中使用$\theta^*$。

正如第6.3节所讨论的，仅关注后验分布的某些统计量（如使后验最大化的参数$\theta^*$）会导致信息丢失，这在使用预测$p(\boldsymbol x\mid\boldsymbol\theta^*)$来做决策的系统中可能是至关重要的。这些决策系统通常具有与似然函数不同的目标函数，例如平方误差损失或分类错误率。因此围绕参数的后验分布可以非常有用，并导致更稳健的决策。贝叶斯推断就是寻找这个后验分布（Gelman等，2004）。对于数据集$\mathcal{X}$、参数先验$p(\boldsymbol{\theta})$和似然函数，后验分布
$$p(\boldsymbol{\theta}\mid\mathcal{X})=\frac{p(\mathcal{X}\mid\boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\mathcal{X})}\:,\quad p(\mathcal{X})=\int p(\mathcal{X}\mid\boldsymbol{\theta})p(\boldsymbol{\theta})\mathrm{d}\boldsymbol{\theta}\:,$$
(8.22)

是通过应用贝叶斯定理获得的。关键思想是利用贝叶斯定理来反转参数$\theta$和数据$\mathcal{X}$（由似然函数给出）之间的关系，以获得后验分布$p(\boldsymbol{\theta}\mid\mathcal{X})$。

参数后验分布的意义在于，它可以用来将参数的不确定性传播到数据上。更具体地说，如果我们有参数上的分布$p(\boldsymbol{\theta})$，那么我们的预测将是
$$p(\boldsymbol{x})=\int p(\boldsymbol{x}\mid\boldsymbol{\theta})p(\boldsymbol{\theta})\mathrm{d}\boldsymbol{\theta}=\mathbb{E}_{\boldsymbol{\theta}}[p(\boldsymbol{x}\mid\boldsymbol{\theta})]\:,$$
(8.23)

并且这些预测不再依赖于模型参数$\theta$，因为$\theta$已经被边缘化/积分掉了。方程(8.23)表明，预测是所有合理参数值$\theta$上的平均值，其中合理性由参数分布$p(\boldsymbol{\theta})$所体现。

在8.3节中讨论了参数估计，并在此处讨论了贝叶斯推断，让我们比较这两种学习方法。通过最大似然估计或最大后验估计（MAP）进行的参数估计会产生参数的一致点估计$\theta^*$，需要解决的关键计算问题是优化。相比之下，贝叶斯推断产生了一个（后验）分布，需要解决的关键计算问题是积分。使用点估计进行预测是直接的，而在贝叶斯框架下进行预测则需要解决另一个积分问题；参见(8.23)。然而，贝叶斯推断为我们提供了一种有原则的方法来整合先验知识、考虑辅助信息，并融入结构知识，这在参数估计的背景下并不容易实现。此外，在数据高效学习的背景下，将参数不确定性传播到预测中对于决策系统中的风险评估和探索非常有价值（Deisenroth等，2015；Kamthe和Deisenroth，2018）。

虽然贝叶斯推断是一个在数学上有原则的参数学习和预测框架，但由于我们需要解决的积分问题，它也存在一些实际挑战；参见(8.22)和(8.23)。更具体地说，如果我们没有为参数选择共轭先验（第6.6.1节），则(8.22)和(8.23)中的积分在解析上不可处理，我们无法以闭式形式计算后验分布、预测或边缘似然。在这些情况下，我们需要诉诸于近似方法。在这里，我们可以使用随机近似，如马尔可夫链蒙特卡洛（MCMC）（Gilks等，1996），或使用确定性近似，如拉普拉斯近似（Bishop，2006；Barber，2012；Murphy，2012）、变分推断（Jordan等，1999；Blei等，2017）或期望传播（Minka，2001a）。

尽管存在这些挑战，但贝叶斯推断已成功应用于各种问题，包括大规模主题建模（Hoffman等，2013）、点击率预测（Graepel等，2010）、控制系统中的数据高效强化学习（Deisenroth等，2015）、在线排名系统（Herbrich等，2007）和大规模推荐系统。还有一些通用工具，如贝叶斯优化（Brochu等，2009；Snoek等，2012；Shahriari等，2016），它们是高效搜索模型或算法元参数时非常有用的组成部分。

备注。在机器学习文献中，“变量”和“参数”之间可能存在一些任意的区分。通常，参数是通过估计得到的（例如，通过最大似然估计），而变量通常会被边缘化。在这本书中，我们对这种区分并不那么严格，因为原则上，我们可以对任何参数设置先验并将其积分出来，这将根据上述区分将该参数转变为随机变量。

### 8.4.3 隐变量模型

在实际应用中，除了模型参数$\theta$外，将额外的隐变量$z$（隐变量）作为模型的一部分是有用的（Moustaki等，2015）。这些隐变量与模型参数$\theta$不同，因为它们不显式地对模型进行参数化。隐变量可能描述数据生成过程，从而有助于模型的可解释性。它们还经常简化模型结构，使我们能够定义更简单且更丰富的模型结构。模型结构的简化通常伴随着模型参数数量的减少（Paquet，2008；Murphy，2012）。隐变量模型中的学习（至少通过最大似然估计）可以通过一种有原则的方式使用期望最大化（EM）算法来完成（Dempster等，1977；Bishop，2006）。在这些隐变量有助于的场景中，例子包括用于降维的主成分分析（第10章）、用于密度估计的高斯混合模型（第11章）、用于时间序列建模的隐马尔可夫模型（Maybeck，1979）或动态系统（Ghahramani和Roweis，1999；Ljung，1999），以及元学习和任务泛化（Hausman等，2018；Sæmundsson等，2018）。虽然引入这些隐变量可能会使模型结构和生成过程变得更简单，但隐变量模型中的学习通常很难，我们将在第11章中看到这一点。
由于隐变量模型还允许我们定义从参数生成数据的过程，让我们来看一下这个生成过程。用$x$表示数据，$\theta$表示模型参数，$z$表示隐变量，我们得到条件分布

(8.24)
$$p(x\mid z,\theta)$$
该分布允许我们为任何模型参数和隐变量生成数据。由于$z$是隐变量，我们对它们放置了一个先验$p(\boldsymbol{z})$。与我们之前讨论的模型一样，具有隐变量的模型可以在我们在8.3节和8.4.2节中讨论的框架内用于参数学习和推断。为了促进学习（例如，通过最大似然估计或贝叶斯推断），我们遵循一个两步过程。首先，我们计算模型的似然$p(\boldsymbol x\mid\boldsymbol{\theta})$，它不依赖于隐变量。其次，我们使用此似然进行参数估计或贝叶斯推断，其中我们分别使用与8.3节和8.4.2节中完全相同的表达式。

由于似然函数$p(x\mid\boldsymbol{\theta})$是在给定模型参数下数据的预测分布，我们需要对隐变量进行边缘化，以便
$$p(\boldsymbol{x}\mid\boldsymbol{\theta})=\int p(\boldsymbol{x}\mid\boldsymbol{z},\boldsymbol{\theta})p(\boldsymbol{z})\mathrm{d}\boldsymbol{z}\:,$$
(8.25)

$\begin{array}{l}\text{其中~}p(x\mid\boldsymbol{z},\boldsymbol{\theta})\text{在(8.24)中给出，且}p(\boldsymbol{z})\text{是隐变量的先验。请注意，似然不应依赖于隐变量}\\\mathrm{v}z\text{，而只是数据}x\text{和模型参数}\theta\text{的函数。}\end{array}$

(8.25)中的似然直接允许通过最大似然估计进行参数估计。如8.3.2节所述，对模型参数$\theta$附加一个先验分布后，MAP估计也变得直接明了。此外，使用(8.25)中的似然，隐变量模型中的贝叶斯推断（8.4.2节）以常规方式进行：我们对模型参数放置一个先验$p(\boldsymbol{\theta})$，并使用贝叶斯定理获得给定数据集$X$之后的后验分布。

(8.26)
$$p(\boldsymbol\theta\mid\mathcal X)=\frac{p(\mathcal X\mid\boldsymbol\theta)p(\boldsymbol\theta)}{p(\mathcal X)}$$
在贝叶斯推断框架内，(8.26)中的后验可用于进行预测；参见(8.23)。在这个隐变量模型中，我们面临的一个挑战是，似然$p(\mathcal{X}\mid\boldsymbol{\theta})$需要根据(8.25)对隐变量进行边缘化。除非我们选择$p(\boldsymbol{x}\mid\boldsymbol{z},\boldsymbol{\theta})$的共轭先验$p(\boldsymbol{z})$，否则(8.25)中的边缘化在解析上不可处理，我们需要求助于近似方法（Bishop，2006；Paquet，2008；Murphy，2012；Moustaki等，2015)

类似于参数后验（8.26），我们可以根据以下公式计算隐变量的后验：
$$p(\boldsymbol{z}\mid\mathcal{X})=\frac{p(\mathcal{X}\mid\boldsymbol{z})p(\boldsymbol{z})}{p(\mathcal{X})}\:,\quad p(\mathcal{X}\mid\boldsymbol{z})=\int p(\mathcal{X}\mid\boldsymbol{z},\boldsymbol{\theta})p(\boldsymbol{\theta})\mathrm{d}\boldsymbol{\theta}\:,$$
其中，$p(\boldsymbol{z})$ 是隐变量的先验，而 $p(\mathcal{X}\mid\boldsymbol{z})$ 需要我们对模型参数 $\theta$ 进行积分。
鉴于解析求解积分的困难性，显然，在一般情况下，同时边缘化隐变量和模型参数是不可能的（Bishop, 2006; Murphy, 2012）。一个更容易计算的量是给定模型参数条件下的隐变量后验分布，即：
$$p(\boldsymbol z\mid\mathcal X,\boldsymbol\theta)=\frac{p(\mathcal X\mid\boldsymbol z,\boldsymbol\theta)p(\boldsymbol z)}{p(\mathcal X\mid\boldsymbol\theta)}\:,$$
(8.28)
其中，$p(\boldsymbol{z})$ 是隐变量的先验，而 $p(\mathcal{X}\mid\boldsymbol{z},\boldsymbol{\theta})$ 在（8.24）中给出。
在第10章和第11章中，我们分别推导了PCA和高斯混合模型的似然函数。此外，我们还计算了PCA和高斯混合模型中隐变量的后验分布（8.28）。
备注。在后续章节中，我们可能不会在隐变量 $z$ 和不确定的模型参数 $\theta$ 之间做出如此清晰的区分，并且也会将模型参数称为“隐变量”或“隐藏变量”，因为它们也是不可观测的。在第10章和第11章中，当我们使用隐变量 $z$ 时，我们会注意到这一点，因为我们将有两种不同类型的隐藏变量：模型参数 $\theta$ 和隐变量 $z$。$\Diamond$
我们可以利用概率模型中所有元素都是随机变量的这一事实，来定义一种统一的表示语言。在第8.5节中，我们将看到一种简洁的图形语言，用于表示概率模型的结构。我们将使用这种图形语言来描述后续章节中的概率模型。

### 8.4.4 进一步阅读

机器学习中的概率模型（Bishop, 2006; Barber, 2012; Murphy, 2012）为用户提供了一种以原则性方式捕获数据和预测模型不确定性的方法。Ghahramani（2015）对机器学习中的概率模型进行了简短的回顾。给定一个概率模型，我们或许足够幸运，能够用解析方法来计算感兴趣的参数。然而，一般来说，解析解是罕见的，因此通常使用计算方法，如采样（Gilks et al., 1996; Brooks et al., 2011）和变分推断（Jordan et al., 1999; Blei et al., 2017）。Moustaki et al.（2015）和Paquet（2008）为潜在变量模型中的贝叶斯推断提供了很好的概述。

近年来，提出了几种编程语言，旨在将软件中定义的变量视为与概率分布相对应的随机变量。其目标是能够编写概率分布的复杂函数，同时在底层由编译器自动处理贝叶斯推断的规则。这个快速发展的领域被称为概率编程。

