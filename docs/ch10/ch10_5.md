## 10.5 高维PCA

为了进行PCA，我们需要计算数据的协方差矩阵。在$D$维空间中，数据协方差矩阵是一个$D\times D$的矩阵。计算这个矩阵的特征值和特征向量在计算上是昂贵的，因为它与$D$的三次方成正比。因此，正如我们之前讨论的那样，PCA在非常高维的情况下是不可行的。例如，如果我们的$x_n$是包含10,0000个像素的图像（例如，$100\times100$像素的图像），那么我们需要计算一个$10,000\times10,000$的协方差矩阵的特征分解。以下，我们针对数据点数量远小于维度的情况（即$N\ll D$）提供了一种解决方案。

假设我们有一个已居中的数据集$x_1,\ldots,x_N$，其中$x_n\in\mathbb{R}^D$。那么，
数据的协方差矩阵定义为
$$S=\frac{1}{N}XX^{\top}\in\mathbb{R}^{D\times D}\:,$$
(10.53)

其中$X=[x_1,\ldots,x_N]$是一个$D\times N$的矩阵，其列是数据点。

我们现在假设$N\ll D$，即数据点的数量小于数据的维度。如果没有重复的数据点，协方差矩阵$S$的秩为$N$，因此它有$D-N+1$个特征值为0。直观地说，这意味着存在一些冗余。接下来，我们将利用这一点，将$D\times D$的协方差矩阵转换为一个$N\times N$的协方差矩阵，其所有特征值都是正的。

在PCA中，我们最终得到了特征向量方程

(10.54)
$$Sb_{m}=\lambda_{m}b_{m}\:,\quad m=1,\ldots,M\:,$$
其中$b_m$是主子空间的一个基向量。让我们重写这个方程：根据(10.53)中定义的$S$，我们得到
$$Sb_{m}=\frac{1}{N}XX^{\top}b_{m}=\lambda_{m}b_{m}\:.$$
(10.55)

我们现在从左侧乘以$X^\top\in\mathbb{R}^{N\times D}$，得到
(10.56)
$$\frac{1}{N}\underbrace{X^{\top}X}_{N\times N}\underbrace{X^{\top}b_{m}}_{=:e_{m}}=\lambda_{m}X^{\top}b_{m}\iff\frac{1}{N}X^{\top}Xc_{m}=\lambda_{m}c_{m}\:,$$

我们得到了一个新的特征向量/特征值方程：$\lambda_m$仍然是特征值，这证实了我们在第4.5.3节中的结果，即$XX^\top$的非零特征值等于$X^\top X$的非零特征值。我们得到与$\lambda_m$相关联的矩阵$\frac1NX^\top X\in\mathbb{R}^{N\times N}$的特征向量为$\boldsymbol{c}_m:=\boldsymbol{X}^\top\boldsymbol{b}_m$。假设我们没有重复的数据点，则该矩阵的秩为$N$且是可逆的。这也意味着$\frac1NX^\top X$与数据协方差矩阵$S$具有相同的（非零）特征值。但现在这是一个$N\times N$的矩阵，因此我们可以比原始的$D\times D$数据协方差矩阵更有效地计算特征值和特征向量。既然我们已经得到了$\frac1NX^\top X$的特征向量，我们接下来将恢复原始的特征向量，这在PCA中仍然需要。目前，我们知道$\frac1NX^\top X$的特征向量。如果我们用$X$左乘我们的特征值/特征向量方程，我们得到

(10.57)
$$\underbrace{\frac{1}{N}XX^\top}_{S}Xc_m=\lambda_mXc_m$$
并且我们再次恢复了数据协方差矩阵。这也意味着我们现在恢复了$Xc_m$作为$S$的一个特征向量。

备注：如果我们想应用我们在第10.6节中讨论的PCA算法，我们需要将$S$的特征向量$Xc_m$归一化，使它们的范数为1。

