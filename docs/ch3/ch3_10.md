## 3.10 拓展阅读

本章我们简要概述了解析几何的一些重要概念，将在本书后续章节中使用。对它们更广泛和深入的概述，我们推荐以下几本优秀的书籍： Axler (2015) 和 Boyd and Vandenberghe (2018)。

内积的存在使我们能用 Gram-Schmidt 正交化方法确定特定向量空间或子空间的基，基向量两两正交。这些基在优化和求解线性方程组的数值算法中非常重要。例如，Krylov 子空间方法、共轭梯度法和广义最小残差方法（generalized minimal residual method，GMRES）它最小化彼此正交的残差误差（Stoer and Burlirsch, 2002）。

在机器学习领域，内积在核方法（Sch&ouml、lkopf and Smola, 2002）中十分很重要。核方法利用了这样一个事实：许多线性算法可以仅通过内积计算来表达。然后，“核技巧”允许我们在（可能是无限维的）特征空间中隐式地计算这些内积，甚至不必明确知道这个特征空间。这使得许多用于机器学习的算法得以“非线性化”，例如用于降维的核PCA（kernel PCA, Schoumlkopf et al., 1997）。同属于核方法的范畴的高斯过程（gaussian process, Rasmussen and Williams, 2006），是概率回归（拟合曲线到数据点）的最新技术。我们将在第12章进一步探讨核的概念。

投影在计算机图形学中经常使用，如用于生成阴影。在优化中，正交投影经常用于（迭代地）最小化残差误差。这也在机器学习中有应用，例如在线性回归中，我们要找到一个（线性）函数，该函数最小化残差误差，即数据到线性函数的正交投影的长度（Bishop, 2006）。我们将在第9章进一步研究这个问题。PCA（Pearson, 1901; Hotelling, 1933）也使用投影来对高维数据降维，它们将在第10章得到更详细地讨论。

