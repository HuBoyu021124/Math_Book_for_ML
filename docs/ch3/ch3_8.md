## 3.8 正交投影

Projections are an important class of linear transformations (besides rotations and reflections) and play an important role in graphics, coding theory, statistics and machine learning. 
投影是一类重要的线性变换（其他重要的线性变换还有旋转和反射），在图形学、编码理论、统计学和机器学习中占有重要的地位。

In machine learning, we often deal with data that is high-dimensional. High-dimensional data is often hard to analyze or visualize. 
在机器学习中，我们经常需要与高维数据打交道，它们往往难以进行分析和可视化。

However, high-dimensional data quite often possesses the property that only a few dimensions contain most information, and most other dimensions are not essential to describe key properties of the data. 
然而，高维数据往往具有大部分信息被包含在仅仅几个维度之中，其他维度对于数据关键信息的刻画并不重要的特点。

When we compress or visualize high-dimensional data, we will lose information. To minimize this compression loss, we ideally find the most informative dimensions in the data. 
当我们对高维数据进行压缩或可视化时，我们将失去一些信息。为了将压缩造成的信息损失最小化，我们往往选择数据中最关键的几个维度。

As discussed in Chapter 1, data can be represented as vectors, and in this chapter, we will discuss some of the fundamental tools for data compression. 
我们在第一章中提到，数据可被表示成向量。在本章中，我们将对基础的数据压缩方法进行讨论。

More specifically, we can project the original high-dimensional data onto a lower-dimensional feature space and work in this lower-dimensional space to learn more about the dataset and extract relevant patterns. 
具体而言，我们可以将原来的高维数据投影到低维**特征空间（feature space）**，然后在此空间中对数据进行处理和分析，以更好的了解数据集并抽取相关的**模式（pattern）**。

> “Feature” is a common expression for data representation.
> “特征”是数据表示的一个常见说法。

For example, machine learning algorithms, such as principal component analysis (PCA) by Pearson (1901) and Hotelling (1933) and deep neural networks (e.g., deep auto-encoders (Deng et al., 2010)), heavily exploit the idea of dimensionality reduction. 
举例来说，以主成分分析（principal component analysis，PCA）为例的机器学习算法（Pearson, 1901 和 Hotelling, 1933）以及以自编码器（auto-encoders，Deng et al., 2010）深度神经网络充分利用了降维的思想。

In the following, we will focus on orthogonal projections, which we will use in Chapter 10 for linear dimensionality reduction and in Chapter 12 for classification. 
接下来，我们将将注意力集中在第十章将被使用于线性降维和在十二章中的分类问题中的正交投影上。

Even linear regression, which we discuss in Chapter 9, can be interpreted using orthogonal projections. 
即使是我们将在第九章中讨论的线性回归算法，也可以从正交投影的角度进行解读。

For a given lower-dimensional subspace, orthogonal projections of high-dimensional data retain as much information as possible and minimize the difference/error between the original data and the corresponding projection. 
给定一个低维子空间，来自高维空间中数据的正交投影会保留尽可能多的信息，并最小化元数据和投影数据的区别或损失。

![400](.../attachments/Pasted%20image%2020240813213937.png)
<center>图 3.9 二维数据点（蓝色点）至一维子空间（直线）的投影（橙色点）</center>

An illustration of such an orthogonal projection is given in Figure 3.9. Before we detail how to obtain these projections, let us define what a projection actually is.
正交投影的直观几何描述可见图 3.9。在我们介紹细节之前，需要首先定义投影这个概念。

> **定义 3.10 （投影）**
> 令 $V$ 为一个向量空间，$U\subset V$ 是 $V$ 的子空间，如果一个线性映射 $\pi: V \rightarrow U$ 满足 $\pi^2 = \pi \circ \pi = \pi$，则称 $\pi$ 为一个**投影（projection）**。

Since linear mappings can be expressed by transformation matrices (see Section 2.7), the preceding definition applies equally to a special kind
of transformation matrices, the projection matrices P π, which exhibit the property that P2 π = P π.
由于线性映射可以表示为矩阵（参见 2.7 节），上面的定义等价于确定了一类特殊的矩阵变换 $P_\pi$，它们满足 $P_\pi^2 = P_\pi$。

In the following, we will derive orthogonal projections of vectors in the inner product space (Rn , h·, ·i) onto subspaces. We will start with one dimensional subspaces, which are also called lines. If not mentioned otherwise, we assume the dot product h x, yi = xTy as the inner product.
在接下来的内容中将推导内积空间 $(\mathbb{R}^n, \langle \cdot, \cdot \rangle)$ 中向量至其子空间的正交投影，我们将从一维子空间（也称为直线）开始。如果没有特殊说明，我们约定向量的内积为点积，即 $\langle x, y \rangle = x^\top y$。

### 3.8.1 向一维子空间（直线）投影

Assume we are given a line (one-dimensional subspace) through the origin with basis vector b ∈ Rn
. The line is a one-dimensional subspace U ⊆ Rn spanned by b. When we project x ∈ Rn onto U, we seek the vector πU (x) ∈ U that is closest to x. Using geometric arguments, let us characterize some properties of the projection πU (x) (Figure 3.10(a)
serves as an illustration):
假设给定一条通过原点的直线（一维子空间），和该空间的一个基 $b \in \mathbb{R}^n$。这条直线是$b$章程的子空间$U \subset \mathbb{R}^n$。当我们将向量$x \in \mathbb{R}^n$投影至$U$中时，我们需要在$U$中寻找距离$x$最近的向量$\pi_U(x) \in U$。下面列举一些投影向量$\pi_U(x)$的性质（参考图 3.10）

<!-- TODO: 需要将其中的英文改成中文 -->

* The projection πU (x) is closest to x, where “closest” implies that the distance k x−πU (x)k is minimal. It follows that the segment πU (x)−x from πU (x) to x is orthogonal to U, and therefore the basis vector b of U. The orthogonality condition yields h πU (x) − x, bi = 0 since angles between vectors are defined via the inner product.
* 投影向量 $\pi_U(x)$ 是（子空间中）距离 $x$ 最近的向量，“最近”的意思是距离 $\|x - \pi_U(x)\|$ 是最小的。这表示从 $\pi_U(x)$ 到 $x$ 的线段 $\pi_U(x) - x$ 与 $U$ 是垂直的，也和 $U$ 的基 $b$ 垂直。

* The projection πU (x) of x onto U must be an element of U and, therefore, a multiple of the basis vector b that spans U. Hence, πU (x) = λb, for some λ ∈ R.
* $x$ 到 $U$ 的投影向量 $\pi_U(x)$ 一定是 $U$ 中的元素，因此也和 $U$ 的基 $b$ 共线。于是存在 $\lambda \in \mathbb{R}$，使得 $\pi_U(x) = \lambda b$。

> Remark. λ is then the coordinate of πU (x) with respect to b.
> 注：$\lambda$ 是 $\pi_{U}(\boldsymbol{x})$ 在基 $b$ 下的坐标。

In the following three steps, we determine the coordinate λ, the projection
πU (x) ∈ U, and the projection matrix P π that maps any x ∈ Rn onto U:
下面我们将通过三个步骤确定坐标 $\lambda$，投影向量 $\pi_{U}(\boldsymbol{x}) \in U$，以及将 $x \in \mathbb{R}^{n}$ 投影至子空间 $U$ 的投影矩阵 $\boldsymbol{P}_{\pi}$ 。

1. Finding the coordinate λ. The orthogonality condition yields
    计算坐标 $\lambda$ 的值。由正交性条件得到
    $$
    \left\langle x - \pi_{U}(\boldsymbol{x}), b \right\rangle = 0 \mathop{\iff}\limits^{\pi_{U}(\boldsymbol{x}) = \lambda b} \left\langle x - \lambda b, b \right\rangle = 0. \tag{3.39}  
    $$
    
    We can now exploit the bilinearity of the inner product and arrive at
    我们可以利用内积的双线性性，得到
    $$
    \left\langle x, b \right\rangle - \lambda\left\langle b, b \right\rangle = 0 \iff \lambda = \frac{\left\langle x, b \right\rangle}{\left\langle b, b \right\rangle } = \frac{\left\langle b, x \right\rangle}{\|b\|^{2}} . \tag{3.40}
    $$

    > With a general inner product, we get λ = h x, bi if k bk = 1.
    > 若使用一般的内积，如果$\|b\| = 1$，我们有 $\lambda = \left\langle x, b \right\rangle$。

    In the last step, we exploited the fact that inner products are symmetric. If we choose h·, ·i to be the dot product, we obtain
    最后，我们利用内积的对称性对原式进行变换。如果我们令 $\left\langle \cdot, \cdot \right\rangle$ 为点积，我们就可以得到
    $$
    \lambda = \frac{b^{\top}x}{b^{\top}b} = \frac{b^{\top}x}{\|b\|^{2}}. \tag{3.41}
    $$

    If k bk = 1, then the coordinate λ of the projection is given by bTx.
    如果 $\|b\| =1$，则 $\lambda$ 的值为 $b^{\top}x$。

2. Finding the projection point πU (x) ∈ U. Since πU (x) = λb, we immediately obtain with (3.40) that 
   计算投影点 $\pi_{U}(\boldsymbol{x}) \in U$。由于 $\pi_{U}(\boldsymbol{x}) = \lambda b$，由 $(3.40)$，立刻有
    $$
    \pi_{U}(\boldsymbol{x}) = \lambda b = \frac{\left\langle x, b \right\rangle}{\|b\|^{2}} \cdot b = \frac{b^{\top}x}{\|b\|^{2}} \cdot b, \tag{3.42}
    $$

    where the last equality holds for the dot product only. We can also compute the length of πU (x) by means of Definition 3.1 as
    其中最后的等号成立条件为内积取为点积。我们还可以根据定义3.1计算 $\pi_U(x)$ 的长度：
    $$
    \|\pi_{U}(\boldsymbol{x})\| = \|\lambda b\| = |\lambda| \|b\|. \tag{3.43}
    $$

    Hence, our projection is of length |λ| times the length of b. This also adds the intuition that λ is the coordinate of πU (x) with respect to the basis vector b that spans our one-dimensional subspace U.
    因此，投影向量的长度为 $|\lambda|$ 乘以 $b$ 的长度。这也增加了一个直观理解方式：$\lambda$ 是投影向量在子空间 $U$ 的基 $b$ 下的坐标。
    
    If we use the dot product as an inner product, we get
    如果我们令内积为点积，就有
    $$
    \begin{align}
    \|\pi_{U}(\boldsymbol{x})\| ~&\mathop{=\!=\!=}\limits^{(3.42)} ~\frac{|b^{\top}x|}{\|b\|^{2}} \|b\|~ \mathop{=\!=\!=}\limits^{(3.25)} ~|\!\cos\omega| \cdot \|x\| \cdot \|b\| \cdot \frac{\|b\|}{\|b\|^{2}} \\&= |\!\cos{\omega}| \cdot \|x\|.
    \end{align} \tag{3.44}
    $$

    
![600](.../attachments/Pasted%20image%2020240813214150.png)
<center>图 3.10 投影至一位子空间的示例。</center>

Here, ω is the angle between x and b. This equation should be familiar from trigonometry: If k xk = 1, then x lies on the unit circle. It follows that the projection onto the horizontal axis spanned by b is exactly cos ω, and the length of the corresponding vector πU (x) = |cos ω|. An
    illustration is given in Figure 3.10(b).
    这里的 $\omega$ 是向量 $x$ 和$b$ 之间的夹角。如图3.10所示，从三角学的角度看，该结果是似曾相识的：如果 $\|x\| = 1$，则向量 $x$ 的终点位于单位圆上。接着可以得到 $x$ 向横轴的投影在基 $b$ 下的坐标恰好就是 $\cos \omega$，投影向量的长度也满足 $|\pi_{U}(\boldsymbol{x})| = |\cos\omega|$。

> The horizontal axis is a one-dimensional subspace.
    > 注：所谓的横轴就是一个一维子空间。


3. Finding the projection matrix P π. We know that a projection is a linear mapping (see Definition 3.10). Therefore, there exists a projection matrix P π, such that πU (x) = P πx. With the dot product as inner product and
    计算投影矩阵 $\boldsymbol{P}_{\pi}$。通过定义 3.10 我们知道投影是一个线性变换。因此存在一个投影矩阵$\boldsymbol{P}_{\pi}$，使得 $\pi_{U}(\boldsymbol{x}) = \boldsymbol{P}_{\pi} x$。若令点积为内积，我们有
    $$
    \pi_{U}(\boldsymbol{x}) = \lambda b = b\lambda =b \frac{b^{\top}x}{\|b\|^{2}} = \frac{bb^{\top}}{\|b\|^{2}} x, \tag{3.45}
    $$

    we immediately see that
    这样立刻得到
    $$
    \boldsymbol{P}_{\pi} = \frac{b b^{\top}}{\|b\|^{2}}. \tag{3.46}
    $$

    Note that bb> (and, consequently, P π) is a symmetric matrix (of rank 1), and kbk2 = h b, bi is a scalar.
    注意 $bb^{\top}$（也就是 $\boldsymbol{P}_{\pi}$）是秩为 $1$ 的对称矩阵，而 $\|b\|^{2} = \left\langle b, b \right\rangle$ 是一个标量。

The projection matrix P π projects any vector x ∈ Rn onto the line through the origin with direction b (equivalently, the subspace U spanned by b).
投影矩阵 $\boldsymbol{P}_{\pi}$ 将任意向量 $x \in \mathbb{R}^{n}$ 投影到通过原点，方向为 $b$ 的直线上（这等价于由 $b$ 张成的子空间 $U$）。

> Remark. The projection πU (x) ∈ Rn is still an n-dimensional vector and not a scalar. However, we no longer require n coordinates to represent the projection, but only a single one if we want to express it with respect to the basis vector b that spans the subspace U: λ. ♦
> 注：投影向量 $\pi_{U}(\boldsymbol{x}) \in \mathbb{R}^{n}$ 依然是一个 $n$ 维向量，不是一个标量。然而，我们不再需要使用 $n$ 个分量来描述它——我们只需要使用一个分量 $\lambda$，因为这是投影向量关于子空间 $U$ 中的基 $b$ 的坐标。

Example 3.10 (Projection onto a Line)
Find the projection matrix P π onto the line through the origin spanned by b = 1 2 2 > . b is a direction and a basis of the one-dimensional subspace (line through origin). With (3.46), we obtain
Let us now choose a particular x and see whether it lies in the subspace
spanned by b. For x =  1 1 1 > , the projection is
Note that the application of P π to πU (x) does not change anything, i.e., P ππU (x) = πU (x). This is expected because according to Definition 3.10, we know that a projection matrix P π satisfies P2π x = Pπ x for all x

> **示例 3.10（向直线投影）**
> 求投影至通过原点，由向量 $b = [1, 2, 2]^{\top}$ 张成直线的投影矩阵 $\boldsymbol{P}_{\pi}$，其中 $b$ 是该过原点直线的方向，也就是一维子空间的基。
> 
> 通过 $(3.46)$，我们有
> $$\boldsymbol{P}_{\pi} = \frac{b b^{\top}}{b^{\top}b} = \frac{1}{9} \left[ \begin{matrix} 1\\2\\2\end{matrix} \right] [1, 2, 2] = \frac{1}{9} \left[ \begin{matrix}1 & 2 &2 \\ 2 & 4 & 4 \\ 2 & 4 & 4\end{matrix}\right] . \tag{3.47}$$
> 现在我们选一个特定的向量 $x$，然后检查它的投影是否在这条直线上。不妨令 $x = [1, 1, 1]^{\top}$，然后计算它的投影：
> $$\pi_{U}(\boldsymbol{x}) = \boldsymbol{P}_{\pi}(x) = \frac{1}{9} \left[ \begin{matrix}1 & 2 &2 \\ 2 & 4 & 4 \\ 2 & 4 & 4\end{matrix} \right] \left[ \begin{matrix}1\\1\\1\end{matrix} \right] = \frac{1}{9 } \left[ \begin{matrix}5\\10\\10\end{matrix} \right] \in \text{span}\left\{ \left[ \begin{matrix}1\\2\\2\end{matrix} \right]  \right\} . \tag{3.48}$$
> 注意，$\boldsymbol{P}_{\pi}$ 作用在 $\pi_{U}(\boldsymbol{x})$ 上的结果等于它本身，这是说 $\boldsymbol{P}_{\pi}\pi_{U}(\boldsymbol{x}) = \pi_{U}(\boldsymbol{x})$。这并不令我们以外，因为根据定义 3.10，我们知道 $\boldsymbol{P}_{\pi}$ 是**幂等**的，也即对于任意的$x$，有 $\boldsymbol{P}_{\pi}^{2}x = \boldsymbol{P}_{\pi}$。

> Remark. With the results from Chapter 4, we can show that πU (x) is an eigenvector of P π, and the corresponding eigenvalue is 1.
> 注：在第四章，我们将证明 $\pi_{U}(\boldsymbol{x})$ 是矩阵 $\boldsymbol{P}_{\pi}$ 的一个特征向量，对应的特征值为 $1$。

### 3.8.2 向一般子空间投影

接下来我们讨论将向量 $x \in \mathbb{R}^{n}$ 投影至较低维度的一般子空间 $U \subset \mathbb{R}^{n}$，其中 $U$ 满足 $\dim U = m \geqslant 1$。如图 3.11 所示。

假设 $(b_{1}, \dots, b_{m})$ 是 $U$ 的一个正交基，$U$ 上的任意投影向量 $\pi_{U}(\boldsymbol{x})$ 必定是它的元素，因此 $U$ 中存在基向量 $b_{1}, \dots, b_{m}$ 的一个线性组合，满足 $\displaystyle \pi_{U}(\boldsymbol{x}) = \sum\limits_{i=1}^{m} \lambda_{i} b_{i}$。

> 注意，如果子空间 $U$ 是通过由一些向量张成的空间而给出的，读者在进行下面的计算之前需要确定其上的一个正交基 $b_{1}, \dots, b_{m}$。

和前文中投影至一维子空间类似，我们按照下面三步就可以找到投影向量 $\pi_{U}(\boldsymbol{x})$ 和投影矩阵 $\boldsymbol{P}_{\pi}$。

1. 确定投影向量在 $U$ 上的基下的坐标 $\lambda_{1}, \dots, \lambda_{m}$，使得下面的线性组合距离 $x \in \mathbb{R}^{n}$ 是最近的。$$\begin{align}\pi_{U}(\boldsymbol{x}) &= \sum\limits_{i=1}^{m} \lambda_{i}b_{i} = \boldsymbol{B\lambda}, \tag{3.49}\\\boldsymbol{B} &= [b_{1}, \dots, b_{m}] \in \mathbb{R}^{n \times m}, \boldsymbol{\lambda} = [\lambda_{1}, \dots, \lambda_{m}]^{\top} \in \mathbb{R}^{m} \tag{3.50}\end{align}$$
   和一维的例子一样，“最近”表示距离最短，这可以推断出连接 $\pi_{U}(\boldsymbol{x})$ 和 $x$ 的向量一定与 $U$ 的所有基向量都垂直（假设内积为点积）。$$\begin{align}\left\langle b_{1}, x - \pi_{U}(\boldsymbol{x}) \right\rangle =&~\, b_{1}^{\top}(x - \pi_{U}(\boldsymbol{x})) = 0, \tag{3.51}\\&\vdots\\\left\langle b_{m}, x - \pi_{U}(\boldsymbol{x}) \right\rangle =&~\, b_{m}^{\top}(x - \pi_{U}(\boldsymbol{x})) = 0, \tag{3.52}\\\end{align}$$
   依据 $\pi_{U}(\boldsymbol{x}) = \boldsymbol{B}\boldsymbol{\lambda}$ 对上式进行替换，有$$\begin{align}b_{1}^{\top}(x& - \boldsymbol{B\lambda}) = 0, \tag{3.53}\\&\vdots\\b_{m}^{\top}(x& - \boldsymbol{B\lambda}) = 0, \tag{3.53}\\\end{align}$$
   这样我们就得到了一个齐次线性方程$$\begin{align}\left[ \begin{matrix}b_{1}^{\top}\\ \vdots \\ b_{m}^{\top}\end{matrix} \right] (x - \boldsymbol{B\lambda}) & \iff \boldsymbol{B}^{\top}(x - \boldsymbol{B\lambda}) = 0 \tag{3.55}\\&\iff  \boldsymbol{B}^{\top}\boldsymbol{B\lambda} = \boldsymbol{B}^{\top}x \tag{3.56}\end{align}$$
   最后得到的方程 $(3.56)$ 叫**正规方程（normal equation）**。由于 $b_{1}, \dots, b_{m}$ 是 $U$ 的一个基，因此它们线性无关，所以矩阵 $\boldsymbol{B}^{\top}\boldsymbol{B} \in \mathbb{R}^{m \times m}$ 是正规矩阵，存在逆矩阵。所以我们可以求得解析解
    $$
    \boldsymbol{\lambda} = (\boldsymbol{B}^{\top}\boldsymbol{B})^{-1}\boldsymbol{B}^{\top}x. \tag{3.57}
    $$
    其中 $(\boldsymbol{B}^{\top}\boldsymbol{B})^{-1}\boldsymbol{B}^{\top}$ 叫矩阵 $\boldsymbol{B}$ 的**伪逆（pseudo-inverse）**，这对不是方形的矩阵也有效，唯一的要求就是 $\boldsymbol{B}^{\top}\boldsymbol{B}$ 是正定的，这表示 $\boldsymbol{B}$ 为**列满秩（full column rank）**。在实际操作中，我们常常对 $\boldsymbol{B}^{\top}\boldsymbol{B}$ 添加一个**摄动项（jitter term）** $\varepsilon \boldsymbol{I}, (\varepsilon > 0)$ 来满足其正定性和数值稳定性。这一对角线上的“山脊”将在第九章中使用Bayesian推断严格推导。
    
    > 译者注：揭示正规矩阵和摄动后的正规矩阵的正定性是显而易见的。任取 $x \in \mathbb{R}^{m}$，构造二次型 $x^{\top}B^{\top}Bx$，立刻有 $x^{\top}B^{\top}Bx = \|Bx\|_{2} \geqslant 0$，由范数的正定性知 $B^{\top}B$ 正定，因此满秩。
    > 对于摄动的情况，类似有 $x^{\top}(B^{\top}B + \varepsilon I)x = \|Bx\|_{2} + \varepsilon\|x\|_{2} > 0$，可知二次型严格大于零，因此摄动后的矩阵必然正定（满秩）。

2. 计算投影向量 $\pi_{U}(\boldsymbol{x}) \in U$。由于我们已经得到 $\pi_{U}(\boldsymbol{x}) = \boldsymbol{B}\boldsymbol{\lambda}$，因此由 $(3.57)$，有 $$\pi_{U}(\boldsymbol{x}) = \boldsymbol{B}(\boldsymbol{B}^{\top}\boldsymbol{B})^{-1}\boldsymbol{B}^{\top}x. \tag{3.58}$$
3. 计算投影矩阵 $\boldsymbol{P}_{\pi}$，从 $(3.58)$ 中我们可以立刻看出方程的解：
$$
\boldsymbol{P}_{\pi} = \boldsymbol{B} (\boldsymbol{B}^{\top}\boldsymbol{B})^{-1}\boldsymbol{B}^{\top}. \tag{3.59}
$$

> 注：上面对于至一般子空间的投影包含了一维的特殊情形。如果 $\dim U = 1$，则 $B^{\top}B \in \mathbb{R}$ 是一个标量，$(3.59)$ 可以被重写成 $\displaystyle \boldsymbol{P}_{\pi} = \frac{B B^{\top}}{B^{\top}B}$，这和 $(3.46)$ 中的矩阵完全一致。

> **示例 3.11（向二维子空间投影）**
> 对于子空间 $\displaystyle U = \text{span} \left\{ \begin{bmatrix} 1\\1\\1 \end{bmatrix}, \begin{bmatrix}0\\1\\2\end{bmatrix}\right\} \subset \mathbb{R}^{3}$ 和向量 $x = \begin{bmatrix}6\\0\\0\end{bmatrix} \in \mathbb{R}^{3}$，找到 $x$ 投影在 $U$ 中的坐标 $\boldsymbol{\lambda}$，投影向量 $\pi_{U}(\boldsymbol{x})$ 以及投影矩阵 $\boldsymbol{P}_{\pi}$
> ****
> 首先，我们检查张成 $U$ 的两个向量，发现它们线性无关，于是可以写成一个矩阵 $\boldsymbol{B} = \begin{bmatrix}1 & 0 \\ 1 & 1 \\ 1 & 2\end{bmatrix}$。
> 然后我们计算正规矩阵和 $\boldsymbol{x}$ 对两个向量的点积：$$\begin{align}\boldsymbol{B}^{\top}\boldsymbol{B} &= \left[ \begin{matrix} 1 & 1 & 1\\0 & 1 & 2\end{matrix} \right] \left[ \begin{matrix} 1 & 0\\1 & 1\\1 & 2\end{matrix} \right] = \left[ \begin{matrix} 3 & 3 \\ 3 & 5\end{matrix} \right], \\\boldsymbol{B}^{\top}x &= \left[ \begin{matrix} 1 & 1 & 1\\0 & 1 & 2\end{matrix} \right] \left[ \begin{matrix} 6\\0\\0\end{matrix} \right] = \left[ \begin{matrix} 6\\0\end{matrix} \right]. \end{align} \tag{3.60}$$
> 第三步，我们解正规方程 $\boldsymbol{B}^{\top}\boldsymbol{B\lambda} = \boldsymbol{B}^{\top}x$ 得到 $\boldsymbol{\lambda}$：$$\left[ \begin{matrix} 3 & 3 \\ 3 & 5\end{matrix} \right] \left[ \begin{matrix} \lambda_{1}\\\lambda_{2}\end{matrix} \right] = \left[ \begin{matrix} 6\\0\end{matrix} \right] \iff \boldsymbol{\lambda} = \left[ \begin{matrix}5\\-3\end{matrix} \right] $$
> 这样依赖，向量 $\boldsymbol{x}$ 投影至子空间 $U$ 的投影向量 $\pi_{U}(\boldsymbol{x})$，也就是向矩阵 $\boldsymbol{B}$ 的列空间投影的向量可以按下式直接进行计算：$$\pi_{U}(\boldsymbol{x}) = \boldsymbol{B\lambda} = \left[ \begin{matrix}5 \\ 2 \\ -1\end{matrix} \right]. \tag{3.62} $$
> 将原来的向量与投影后的向量作差得到向量的长度就是**投影损失（projection error）**：$$\|x - \pi_{U}(\boldsymbol{x})\| = \Big\|[1, -2, 1]^{\top}\Big\| = \sqrt{ 6 }. \tag{3.63}$$
> 相应地，对于任意 $\boldsymbol{x} \in \mathbb{R}^{3}$ 的投影矩阵 $\boldsymbol{P}_{\pi}$ 由下式给出：$$\boldsymbol{P}_{\pi} = \boldsymbol{B} (\boldsymbol{B}^{\top}\boldsymbol{B})^{-1}\boldsymbol{B}^{\top} =- \frac{1}{6}\left[ \begin{matrix}5 & 2 & -1\\2 & 2 & 2\\-1&2&5\end{matrix} \right]. \tag{3.64}$$
> 我们可以通过验证残差向量 $x - \pi_{U}(\boldsymbol{x})$ 是否和所有 $U$ 的基垂直并考察 $\boldsymbol{P}_{\pi}^{2} = \boldsymbol{P}_{\pi}$ （参见定义 3.10）是否成立来验证计算结果的正确性。

> 注1：投影向量 $\pi_{U}(\boldsymbol{x})$ 虽然在子空间 $U \subset \mathbb{R}^{m}$ 中，但它依然是 $\mathbb{R}^{n}$ 中的向量。但我们只需用 $U$ 中关于基向量 $b_{1}, \dots, b_{m}$ 的坐标 $\lambda_{1}, \dots, \lambda_m$ 来表示它就足够了。

> 注2：在使用一般内积定义的向量空间中，我们在通过内积计算向量之间的夹角和距离是需要额外注意。

投影可以让我们对无解的线性系统 $\boldsymbol{Ax}  =\boldsymbol{b}$ 进行研究。让我们回忆 $\boldsymbol{b}$ 不在 $\boldsymbol{A}$ 张成的空间，也就是 $\boldsymbol{A}$ 所有列张成的空间（列空间）中的情形。在给出这样一个无解的线性系统时，我们可以找到一个**近似解**，也就是 $\boldsymbol{A}$ 的列空间中最接近 $\boldsymbol{b}$ 的向量。换句话说，我们计算 $\boldsymbol{b}$ 到 $\boldsymbol{A}$ 的列空间的投影，就是所求的近似解。这种问题在实作中非常常见，其得到的结果叫做超定系统（over-determined system）的**最小二乘估计（least-squares solution）**，类似地问题将在 9.4 节中继续讨论。如果再引入**重构损失（reconstruction error）**，就构成了推导主成分分析（10.3 节）的一种方式。

> 注：前文中我们只要求 $\{ \boldsymbol{b}_{1}, \dots, \boldsymbol{b}_{k} \}$ 是子空间 $U$ 的一个基，如果它是标准正交基，则 $(3.33)$ 和 $(3.34)$ 可以被用来化简 $(3.58)$。由于 $\boldsymbol{B}^{\top}\boldsymbol{B} = \boldsymbol{I}$，我们可以得到下面更加简洁的投影表达式：$$\pi_{U}(\boldsymbol{x}) = \boldsymbol{B B}^{\top}x \tag{3.65}$$
> 以及坐标 $\boldsymbol{\lambda}$ ：$$\boldsymbol{\lambda} = \boldsymbol{B}^{\top}x. \tag{3.66}$$
> 这意味着我们不再需要进行耗时的求逆计算了。

### 3.8.3 Gram-Schmidt 正交化

投影是 Gram-Schmidt正交化的核心，后者让我们可以从任意的 $n$ 维向量空间 $V$ 的一个基 $(\boldsymbol{b}_{1}, \dots, \boldsymbol{b}_{n})$ 构造出该空间的一个标准正交基 $(\boldsymbol{u}_{1}, \dots, \boldsymbol{u}_{n})$ 。这个正交基总是存在，且满足 $\text{span}\{ \boldsymbol{b}_{1}, \dots, \boldsymbol{b}_{n} \} = \text{span}\{ \boldsymbol{u}_{1}, \dots, \boldsymbol{u}_{n} \}$。所谓的 Gram-Schmidt 正交化方法在给定 $V$ 的任意基 $(\boldsymbol{b}_{1}, \dots, \boldsymbol{b}_{n})$ 的情况下迭代地构造出正交基 $(\boldsymbol{u}_{1}, \dots, \boldsymbol{u}_{n})$，其过程如下：
$$
\begin{align}
\boldsymbol{u}_{1} &:= \boldsymbol{b}_{1}, \tag{3.67}\\
\boldsymbol{u}_{k} &:= \boldsymbol{b}_{k} - \pi_{\text{span}\{ \boldsymbol{u}_{1}, \dots, \boldsymbol{u}_{k-1} \}}(\boldsymbol{b}_{k}), \quad k = 2, \dots, n.
\end{align}
$$
在式 $(3.68)$ 中，第 $k$ 个基向量 $\boldsymbol{b}_{k}$ 被投影至前 $k-1$ 个构造得到的单位正交向量 $\boldsymbol{u}_{1}, \dots, \boldsymbol{u}_{k-1}$ 张成的子空间上（参见 3.8.2 节）。向量 $\boldsymbol{b}_{k}$ 减去这个投影向量所得的向量 $\boldsymbol{u}_{k}$ 与 $\boldsymbol{u}_{1}, \dots, \boldsymbol{u}_{k-1}$ 张成的 $k-1$ 维子空间垂直。对所有 $n$ 个基向量 $\boldsymbol{b}_{1}, \dots, \boldsymbol{b}_{n}$ 逐个应用这个算法，就得到了空间 $V$ 的一个正交基 $(\boldsymbol{u}_{1}, \dots, \boldsymbol{u}_{n})$ 。如果我们将正交基中的向量全部标准化，使得对所有的 $k = 1, \dots, n$ 都有 $\|\boldsymbol{u}_{k}\| = 1$，我们就得到了原空间的一个标准正交基（ONB）。

> **示例 3.12（Gram-Schmidt 正交化）**
> ![](.../attachments/Pasted%20image%2020240817143850.png)
> <center>图 3.12 Gram-Schmidt 正交化</center>
> 如图 3.12 所示，考虑 $\mathbb{R}^{2}$ 的一个基 $(\boldsymbol{b}_{1}, \boldsymbol{b}_{2})$，其中
> $$\boldsymbol{b}_{1} = \begin{bmatrix}2\\0\end{bmatrix}, \quad \boldsymbol{b}_{2} = \begin{bmatrix}1\\1\end{bmatrix}; \tag{3.69}$$
> 使用 Gram-Schmidt 正交化方法，我们可按照下面的过程构造 $\mathbb{R}^{2}$ 的一个正交基：$$\begin{align}\boldsymbol{u}_{1} &= \boldsymbol{b}_{1} = \begin{bmatrix}2\\0\end{bmatrix},\tag{3.70}\\\boldsymbol{u}_{2} &= \boldsymbol{b_{2}} - \pi_{\text{span}\{ \boldsymbol{u}_{1} \}}(\boldsymbol{b}_{2}) \\ &\,\mathop{=\!=\!=}\limits^{(3.45)} \,\boldsymbol{b}_{2} - \frac{\boldsymbol{u}_{1}\boldsymbol{u}_{1}^{\top}}{\|\boldsymbol{u}_{1}\|^{2}}\cdot\boldsymbol{b}_{2} = \begin{bmatrix}1\\1\end{bmatrix} - \begin{bmatrix}1&0\\0&0\end{bmatrix}\begin{bmatrix}1\\1\end{bmatrix} = \begin{bmatrix}0\\1\end{bmatrix}. \tag{3.71}\end{align}$$
> 上面的步骤对应图 3.12 中的 (b) 和 (c)。我们可以立即看出 $\boldsymbol{u}_{1}$ 和 $\boldsymbol{u}_{2}$ 是垂直的，也即 $\boldsymbol{u}_{1} ^\top \boldsymbol{u}_{2} = 0$。

### 3.8.4 向仿射子空间投影

直到现在我们讨论的都是如何讲一个向量投影到低维的子空间 $U$ 上。本节将讨论如何解决投影至仿射子空间的问题。

![](.../attachments/Pasted%20image%2020240817145017.png)
<center>图 3.13 向仿射空间投影</center>

考虑像图 3.13(a) 这样的问题：给定一个仿射空间 $L = \boldsymbol{x}_{0} + U$，其中 $\boldsymbol{b}_{1}, \boldsymbol{b_{2}}$ 是 $U$ 的一个基。为确定向量 $\boldsymbol{x}$ 到仿射空间 $L$ 的投影 $\pi_{L}(\boldsymbol{x})$，我们选择将其转换为我们已解决的投影至低维子空间的问题。我们对 $\boldsymbol{x}$ 和 $L$ 同时减去支持点 $\boldsymbol{x}_{0}$，这样一来 $L - \boldsymbol{x}_{0}$ 恰好就是子空间 $U$。这样我们可以使用前文中 3.8.2 节讨论的正交投影至子空间的方法得到 $\pi_{U}(\boldsymbol{x} - \boldsymbol{x}_{0})$（如图 3.13 (b) 所示），然后我们可以把 $\boldsymbol{x}_{0}$ 加回投影向量，将它重新放入 $L$ 中，这样我们就得到了 $\boldsymbol{x}$ 到 $L$ 的投影：
$$
\pi_{L}(\boldsymbol{x}) = \boldsymbol{x}_{0} + \pi_{U}(\boldsymbol{x} - \boldsymbol{x}_{0}), \tag{3.72}
$$
其中 $\pi_{U}(\cdot)$ 是至子空间 $U$ 的投影，也就是 $L$ 的方向空间（如图3.13所示）。

从图可以看出，从 $\boldsymbol{x}$ 到 $L$ 的距离和 $\boldsymbol{x} - \boldsymbol{x}_{0}$ 到 $U$ 的距离相等，也就是
$$
\begin{align}
d(\boldsymbol{x}, L) &= \|\boldsymbol{x} - \pi_{L}(\boldsymbol{x})\| = \|\boldsymbol{x} - [\boldsymbol{x}_{0} + \pi_{U}(\boldsymbol{x} - \boldsymbol{x}_{0})]\| \tag{3.73a}\\[0.2em]
&= d(\boldsymbol{x} - \boldsymbol{x}_{0}, \pi_{U}(\boldsymbol{x} - \boldsymbol{x}_{0})) = d(\boldsymbol{x} - \boldsymbol{x}_{0}, U). \tag{3.73b}
\end{align}
$$
在 12.1 节，我们将会用这个方法导出**分割超平面**这个概念。

