# 第十一章 密度估计和 Gauss 混合模型

在前面的章节中，我们已经介绍了机器学习中的两个基本问题：回归（第9章）和降维（第10章）。在本章中，我们将探讨机器学习的第三大支柱：密度估计。在这个过程中，我们将引入一些重要的概念，例如期望最大化（EM）算法，以及从潜在变量的角度看待使用混合模型进行密度估计。
当我们将机器学习应用于数据时，我们通常希望以某种方式表示数据。一种直接的方法是将数据点本身作为数据的表示；图11.1给出了一个示例。然而，如果数据集非常大，或者我们对表示数据的特征感兴趣，那么这种方法可能就不太有用了。在密度估计中，我们使用参数族中的一个密度函数（例如高斯分布或贝塔分布）来紧凑地表示数据。例如，我们可能会寻找数据集的均值和方差，以便使用高斯分布来紧凑地表示数据。均值和方差可以使用我们在8.3节中讨论过的工具来找到：最大似然估计或最大后验估计。然后，我们可以使用这个高斯分布的均值和方差来表示数据背后的分布，也就是说，如果我们从这个分布中采样，我们会认为这个数据集是这个分布的一个典型实现。

<center>
<img src="./attachments/ch11_2d-mixed_set.png" alt="alt text" style="zoom:50%;">
</center>
<center>
</center>

在实践中，高斯分布（或者到目前为止我们遇到的所有其他分布）的建模能力是有限的。例如，用高斯分布来近似图11.1中数据的密度将是一个很差的近似。接下来，我们将研究一类更具表达能力的分布，我们可以用它们来进行密度估计：混合模型。

**混合模型**
混合模型可以通过K个简单（基础）分布的凸组合来描述一个分布$p(x)$：

$$
\begin{align}
p(\boldsymbol{x}) \leqslant \sum\limits_{k=1}^{K} \pi_{k}p_{k}(\boldsymbol{x})\tag{11.1}\\
0 \leqslant \pi_{k} \leqslant 1, \quad \sum\limits_{k=1}^{K} \pi_{k} = 1, \tag{11.2}
\end{align}
$$

其中，分量$p_{k}$是基本分布族的成员，例如高斯分布、伯努利分布或伽马分布，而$\pi_{k}$是混合权重。混合模型比相应的基础分布更具表达能力，因为它们允许对多峰数据进行表示，也就是说，它们可以描述具有多个“簇”的数据集，如图11.1中的示例。

**混合权重**
我们将重点关注高斯混合模型（GMM），其中基本分布是高斯分布。对于给定的数据集，我们的目标是最大化模型参数的似然，以训练GMM。为此，我们将使用第5章、第6章和7.2节中的结果。然而，与我们之前讨论过的其他应用（线性回归或主成分分析）不同，我们不会找到一个封闭形式的最大似然解。相反，我们将得到一组相互依赖的联立方程，我们只能迭代地求解它们。


