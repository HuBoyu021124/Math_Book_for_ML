# 1.2 阅读本书的两种方法

我们可以考虑采用两种策略来理解机器学习的数学知识：~~一种是将数学知识应用于机器学习，另一种是将数学知识应用于机器学习~~

- 自下而上： 从基础概念到高级概念。在数学等技术性较强的领域，这通常是首选方法。这种策略的好处是，读者在任何时候都能依靠以前学过的概念。遗憾的是，对于实践者来说，许多基础概念本身并不特别有趣，而且缺乏动力，这意味着大多数基础定义很快就会被遗忘。
- 自上而下： 从实际需求深入到更基本的要求。这种以目标为导向的方法的优点是，读者随时都知道他们为什么需要学习某个特定的概念，而且所需知识的路径也很清晰。这种策略的缺点是，知识可能建立在不稳固的基础上，读者必须记住一系列他们根本无法理解的单词。

我们决定以模块化的方式编写本书，将基础（数学）概念与应用分开，这样就可以从两个方面阅读本书。本书分为两部分，第一部分奠定数学基础，第二部分将第一部分的概念应用于一系列基本的机器学习问题，如图 1.1 所示，这些问题构成了机器学习的四大支柱：回归、降维、密度估计和分类。第一部分的章节大多建立在前一章的基础上，但如果有必要，也可以跳过一章，向后学习。第二部分各章之间的联系并不紧密，可以按照任意顺序阅读。有许多向前和向后的指向。


<center>
<img src="./attachments/1.1.png" alt="机器学习的基础和四大支柱" style="zoom:80%;">
</center>

<center>图1.1 机器学习的基础和四大支柱</center>
​


当然，阅读本书的方法不止两种。大多数读者会采用自上而下和自下而上相结合的方法来学习，有时会先积累基本的数学技能，然后再尝试更复杂的概念，但也会根据机器学习的应用来选择主题。



## 第一部分 关于数学

我们在本书中介绍的机器学习四大支柱（见图 1.1）需要坚实的数学基础，这将在第一部分中阐述。

我们用向量来表示数值数据，用矩阵来表示这些数据的表格。对向量和矩阵的研究称为线性代数，我们将在第 2 章介绍线性代数。该章还介绍了将向量集合为矩阵的方法。

给定代表现实世界中两个物体的两个向量，我们要对它们的相似性做出说明。我们的想法是，我们的机器学习算法（我们的预测器）应该预测相似的向量会有相似的输出。为了使向量间的相似性概念正规化，我们需要引入一些操作，将两个向量作为输入，并返回一个代表其相似性的数值。相似性和距离的构造是解析几何的核心，将在第 3 章中讨论。

在第 4 章中，我们将介绍有关矩阵和矩阵分解的一些基本概念。矩阵的一些运算在机器学习中非常有用，可以直观地解释数据，提高学习效率。

我们通常认为数据是对某些真实潜在信号的噪声观测。我们希望通过应用机器学习，从噪声中识别出信号。这就要求我们有一种语言来量化 “噪声 ”的含义。我们通常还希望预测器能让我们表达某种不确定性，例如，量化我们对特定测试数据点的预测值的信心。不确定性的量化属于概率论的范畴，将在第 6 章中介绍。

为了训练机器学习模型，我们通常要找到能最大化某些性能指标的参数。许多优化技术都需要梯度的概念，它告诉我们寻找解决方案的方向。第 5 章是关于向量微积分的内容，详细介绍了梯度的概念，随后我们将在第 7 章中使用梯度的概念来进行优化，找到函数的最大值/最小值。



## 第二部分 关于机器学习

本书第二部分介绍了机器学习的四大支柱，如图 1.1 所示。我们说明了本书第一部分介绍的数学概念是如何为每个支柱奠定基础的。大体上，各章按难度排序（从高到低）。

在第8章中，我们以数学的方式重述了机器学习的三个组成部分（数据、模型和参数估计）。此外，我们还提供了一些建立实验装置的指南，以防止对机器学习系统的评估过于乐观。回顾一下，我们的目标是建立一个在未见数据上表现良好的预测器。

在第 9 章中，我们将仔细研究线性回归，我们的目标是找到将输入 $\boldsymbol{x} \in \mathbb{R}^D$ 映射到或对应的观测函数值 $y \in \mathbb{R}$ 的函数，我们可以将其解释为各自输入的标签。我们将讨论通过最大似然估计和最大后验估计进行的经典模型拟合（参数估计），以及贝叶斯线性回归，在贝叶斯线性回归中，我们对参数进行积分而不是优化。

第 10 章的重点是利用主成分分析法降维，即图 1.1 中的第二个支柱。降维的主要目的是为高维数据 $\boldsymbol{x} \in \mathbb{R}^D$ 找到一个紧凑的低维表示，它通常比原始数据更容易分析。与回归不同的是，降维只关注数据建模--数据点 $x$ 没有相关标签。

在第 11 章中，我们将转向第三个支柱：密度估计。密度估计的目标是找到描述给定数据集的概率分布。为此，我们将重点关注高斯混合模型，并讨论一种迭代方案来找到该模型的参数。与降维一样，数据点 $\boldsymbol{x} \in \mathbb{R}^D$ 没有相关标签。然而，我们并不寻求数据的低维表示。相反，我们感兴趣的是能描述数据的密度模型。

第 12 章以深入讨论第四个支柱：分类作为本书的结尾。我们将在支持向量机的背景下讨论分类。与回归（第 9 章）类似，我们有输入 $x$ 和相应的标签 $y$ 。然而，与回归不同的是，回归中的标签是实值，而分类中的标签是整数，这就需要特别注意。