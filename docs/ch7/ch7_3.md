## 7.3 凸优化
我们将目光聚焦于一类能**保证全局最优解**的特殊优化问题。当目标函数 $f(\cdot)$ 是凸函数，且约束函数 $g(\cdot)$ 和 $h(\cdot)$ 定义的集合为凸集时，这类问题称为**凸优化问题**。凸优化问题具有 **强对偶性**：对偶问题的最优解与原问题完全一致。虽然机器学习文献常模糊凸函数与凸集的界限，但上下文通常能提供明确指引。 

> **定义 7.2（凸集）** 
> 若集合 $\mathcal{C}$ 满足：对任意 $x, y\in \mathcal{C}$ 和标量 $\theta \in [0,1]$，有 $$\theta x + (1-\theta)y \in \mathcal{C}. \tag{7.29}$$ 则称 $\mathcal{C}$ 为凸集。

凸集中两点之间的线段总是位于凸集中。下图给出了凸集的一个典型例子和反例。

<center><img src="ch7/attachments/Pasted%20image%2020250701182000.png" alt="alt text" style="zoom:50%;"></center>
<center>图 7.5, 图 7.6 凸集（左）和非凸集合（右）</center>
凸函数定义和凸集很像，它的定义是函数上两点的连线一定位于函数曲线的上方。图 7.2 画了一个非凸函数，图 7.3 画的是凸函数。图 7.7 中也是一个凸函数。

> **定义 7.3（凸函数）**
> 考虑函数 $f: \mathbb{R}^{D} \rightarrow \mathbb{R}$，且 $f$ 的定义域为凸集。则它被称为是 **凸函数** 如果对定义域中的所有 $\boldsymbol{x}, \boldsymbol{y}$ 和任意标量 $0 \leqslant \theta \leqslant 1$，都有$$f(\theta \boldsymbol{x} + (1-\theta)\boldsymbol{y}) \leqslant \theta f(\boldsymbol{x}) + (1-\theta)f(\boldsymbol{y}).\tag{7.30}$$

> **注释**
> 一个 **凹函数** 一定是某个 **凸函数** 的负数

公式 (7.28) 中的约束通过限制约束函数 $f(\cdot)$ 和 $g(\cdot)$ 的标量函数值，最终得到一个集合 —— 可行域。凸函数和凸集之间的另一种关系是考虑通过 “填充” 凸函数得到的集合。凸函数是一个 “碗状物体”，我们想象往里面倒水来填满它。这个填满的集合被称为凸函数的 **上镜图 (epigraph)**，它也是一个凸集。

如果函数 $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$ 是可微的，我们还可以根据其梯度 $\nabla_{\boldsymbol{x}}f(\boldsymbol{x})$（见 5.2）来判断其凸性。这样的函数是凸的，当且仅当对任意定义域中的 $\boldsymbol{x}$ 和 $\boldsymbol{y}$，都有
$$f(\boldsymbol{y}) \geqslant f(\boldsymbol{x}) + \nabla_{\boldsymbol{x}}f(\boldsymbol{x})^{\top}(\boldsymbol{y} - \boldsymbol{x}). \tag{7.31}$$
进一步地，如果我们知道 $f$ 是二阶可微的，也就是在定义域中的每一点都存在 Hesse 矩阵 (5.147)，则该函数是凸的当且仅当 $\nabla_{\boldsymbol{x}}^{2}f(\boldsymbol{x})$ 是半正定的 (Boyd and Vandengerghe, 2004)。

> **示例 7.3（熵）**
> 
> 负熵函数 $f(x) = x\log_{2}x$ 在 $x > 0$ 上是凸函数，如图 7.8 所示。为了说明先前提到的凸函数的定义，我们选择 $x = 2$ 和 $x = 4$ 两个位置检查。需要注意的是，要证明该函数的凸性，只选择两个点不够，我们要检查所有的 $x \in \mathbb{R}$。
> 
> <center><img src="ch7/attachments/Pasted%20image%2020250701164942.png" alt="alt text" style="zoom:50%;"></center>
> 
> 让我们回忆定义 7.3，考虑它们的中间位置 ( $\theta = 0.5$ )，那么公式 (7.30) 的左边是 $f(0.5 \cdot 2 + 0.5 \cdot 4) = 3\log_{2}3 \approx 4.75$，右边是 $0.5(2\log_{2}2) + 0.5(4\log_{2}4) = 1 + 4 = 5$，这符合凸函数的定义。
> 由于 $f(x)$ 是可微的，我们也可以使用公式 (7.31) 对其图形进行判定。首先我们计算 $f(x)$ 的导数：$$\nabla_{x}\log(x\log_{2}x) = 1 \cdot \log_{2}x + x \cdot \frac{1}{x\log_{e}2} = \log_{2}x + \frac{1}{\log_{e}2}. \tag{7.32}$$ 我们同样使用 $x = 2$ 和 $x = 4$ 两点，公式 (7.31) 的左侧是 $f(4) = 8$，右侧是 $$\begin{align}f(\boldsymbol{x}) + \nabla_{\boldsymbol{x}}^{\top}(\boldsymbol{y} - \boldsymbol{x}) &= f(2) + \nabla f(2) \cdot (4 - 2) \tag{7.33a}\\&= 2 + \left( 1 + \frac{1}{\log_{e}2} \right) \cdot 2 \approx \frac{6}{9} \tag{7.33b}\end{align}$$

我们可以通过多种方法检查一个函数是否是凸函数。实际操作中我们通常通过保持凸性的变换来检查某个函数或集合是不是凸的。尽管细节有很大不同，但这仍然是我们在第二章中为线性空间引入的闭包思想。

> **示例 7.4（凸函数的非负线性组合）**
> 若干凸函数的非负线性组合还是凸函数。我们首先观察到，如果 $f$ 是凸函数，那么对于任意非负实数 $\alpha$，函数 $\alpha f$ 也是凸的。这个证明很简单，只需要将公式 (7.3) 的左右两侧都乘上 $\alpha$ 即可。
> 考虑两个凸函数 $f_{1}$ 和 $f_{2}$，根据凸函数定义我们有 $$\begin{align}f_{1}(\theta \boldsymbol{x} + (1 - \theta)\boldsymbol{y}) &\leqslant \theta f_{1}(\boldsymbol{x}) + (1-\theta)f_{1}(\boldsymbol{y}) \tag{7.34}\\f_{2}(\theta \boldsymbol{x} + (1 - \theta)\boldsymbol{y}) &\leqslant \theta f_{2}(\boldsymbol{x}) + (1-\theta)f_{2}(\boldsymbol{y}). \tag{7.35}\end{align}$$ 两式相加，有$$\begin{align}f_{1}(\theta \boldsymbol{x} &+ (1 - \theta)\boldsymbol{y}) + f_{2}(\theta \boldsymbol{x} + (1 - \theta)\boldsymbol{y}) \\ &\leqslant \theta f_{1}(\boldsymbol{x}) + (1-\theta)f_{1}(\boldsymbol{y}) + \theta f_{2}(\boldsymbol{x}) + (1-\theta)f_{2}(\boldsymbol{y}) \end{align}\tag{7.36}$$ 其中不等式右边还可以进一步整理为 $$\theta \Big[ f_{1}(\boldsymbol{x}) + f_{2}(\boldsymbol{x}) \Big] + (1-\theta)\Big[ f_{1}(\boldsymbol{y}) + f_{2}(\boldsymbol{y}) \Big] , \tag{7.37}$$ 这样我们就证明了 $f_{1} + f_{2}$ 是凸的。结合这两个事实，我们有对于任意的 $\alpha, \beta \geqslant 0$，$\alpha f_{1} + \beta f_{2}$ 是凸函数。对于三个及以上函数的非负线性组合，证明方法类似。

> **注释**
> 公式 (7.30) 中的不等式又称为 **Jensen 不等式**。事实上，这一整类用于求凸函数非负加权和的不等式都称为 Jensen 不等式。

总的来说，被称为 **凸优化** 的约束优化问题的长相如下：
$$\begin{align}\min\limits_{\boldsymbol{x}}~&~f(\boldsymbol{x})\\\text{subject to}~&~g_{i}(\boldsymbol{x}) \leqslant 0\quad \forall i = 1, \dots, m\\&~h_{j}(\boldsymbol{x}) = 0\quad \forall j = 1, \dots, n,\end{align}\tag{7.38}$$
其中 $f(\boldsymbol{x})$ 和所有的 $g_{i}(\boldsymbol{x})$ 都是凸函数，所有的 $h_{j}(\boldsymbol{x}) = 0$ 都对应着凸集。下面的内容我们将讨论两个常用并以研究透了的凸优化问题。

### 7.3.1 线性规划
我们首先考虑所有函数都是线性函数这一特殊情况：
$$\begin{align}\min\limits_{\boldsymbol{x} \in \mathbb{R}^{d}}~&~\boldsymbol{c}^{\top}\boldsymbol{x}\\\text{subject to}~&~\boldsymbol{A}\boldsymbol{x} \leqslant \boldsymbol{b},\end{align}\tag{7.39}$$
其中 $\boldsymbol{A} \in \mathbb{R}^{m \times d}$，$\boldsymbol{b} \in \mathbb{R}^{m}$。这样的问题称为 **线性规划**。

> **注释**
> 线性规划是工业中最常用的一类方法 

它有 $d$ 个变量和 $m$ 个线性约束，它的 Lagrangre 函数是
$$\mathfrak{L}(\boldsymbol{x}, \boldsymbol{\lambda}) = \boldsymbol{c}^{\top}\boldsymbol{x} + \boldsymbol{\lambda}^{\top}(\boldsymbol{A}\boldsymbol{x} - \boldsymbol{b}), \tag{7.40}$$
其中 $\boldsymbol{\lambda} \in \mathbb{R}^{m}$ 是非负的 Lagrangre 乘子组成的向量，稍微整理一下，得到
$$\mathfrak{L}(\boldsymbol{x}, \boldsymbol{\lambda}) = (\boldsymbol{c} + \boldsymbol{A}^{\top}\boldsymbol{\lambda})^{\top}\boldsymbol{x} - \boldsymbol{\lambda}^{\top}\boldsymbol{b}. \tag{7.41}$$
求 $\mathfrak{L}(\boldsymbol{x}, \boldsymbol{\lambda})$ 对 $\boldsymbol{x}$ 的导数，并令其为零，我们得到
$$\boldsymbol{c} + \boldsymbol{A}^{\top}\boldsymbol{\lambda} = \boldsymbol{0}. \tag{7.42}$$
因此我们得到对偶 Lagrangre 函数 $\mathfrak{D}(\boldsymbol{\lambda}) = -\boldsymbol{\lambda}^{\top}\boldsymbol{b}$，我们需要最大化 $\mathfrak{D}(\boldsymbol{\lambda})$。除了前文中 $\mathfrak{L}(\boldsymbol{x}, \boldsymbol{\lambda})$ 要为零，我们还需要保持 $\boldsymbol{\lambda} \geqslant \boldsymbol{0}$，这就得到下面的对偶优化问题
$$\begin{align}\max\limits_{\boldsymbol{\lambda} \in \mathbb{R}^{m}}~&~-\boldsymbol{b}^{\top}\boldsymbol{\lambda}\\\text{subject to}~&~\boldsymbol{c} + \boldsymbol{A}^{\top}\boldsymbol{\lambda} = \boldsymbol{0}\\&~\boldsymbol{\lambda} \geqslant \boldsymbol{0}\end{align} \tag{7.43}$$
> **注解**
> 一般主问题是一个最小化的问题，对偶问题则是一个最大化的问题。

这还是一个线性优化问题，但变元的数量是 $m$。我们可以依据实际情况选择是解原问题 (7.39) 还是解对偶问题 (7.43)，就看是原问题中的变元数量 $d$ 更小还是原问题中约束数量 $m$ 更小，哪个小选哪个。

> **示例 7.5（线性规划）**
> 考虑下面的二变元线性规划问题 $$\begin{align}\min\limits_{\boldsymbol{x} \in \mathbb{R}^{2}}~&~\begin{bmatrix}5\\3\end{bmatrix}^{\top}\begin{bmatrix}x_{1}\\x_{2}\end{bmatrix}\\[0.5em]\text{subject to}~&~\begin{bmatrix}2&2\\2&-4\\-2&1\\0&-1\\0&1\end{bmatrix}\begin{bmatrix}x_{1}\\x_{2}\end{bmatrix} \leqslant \begin{bmatrix}33\\8\\5\\-1\\8\end{bmatrix}\end{align} \tag{7.44}$$如图 7.9。
> <center><img src="ch7/attachments/Pasted%20image%2020250701172750.png" alt="alt text" style="zoom:50%;"></center>
> 由图可知目标函数是线性的 —— 它的等高线是直线。问题的约束集合在图中由不同颜色的实直线表示，可行域由灰色阴影表示，这意味着最优解（红色五角星）必须在灰色阴影区域（在此例中，也包括其边缘）。

### 7.3.2 二次规划
现在考虑目标函数是凸的二次函数，而约束是仿射函数的情形：
$$\begin{align}\min\limits_{\boldsymbol{x} \in \mathbb{R}^{d}}~&~ \frac{1}{2}\boldsymbol{x}^{\top}\boldsymbol{Q}\boldsymbol{x} + \boldsymbol{c}^{\top}\boldsymbol{x}\\\text{subject to}~&~\boldsymbol{A}\boldsymbol{x} \leqslant \boldsymbol{b},\end{align}\tag{7.45}$$
其中 $\boldsymbol{A} \in \mathbb{R}^{m \times d}, \boldsymbol{b} \in \mathbb{R}^{m}, \boldsymbol{c} \in \mathbb{R}^{d}$。目标函数中的矩阵 $\boldsymbol{Q} \in \mathbb{R}^{d \times d}$ 是正定的，因此目标函数是凸的。这样的问题叫做 **二次规划**。它有 $d$ 个变量， $m$ 个线性约束。

> **示例 7.6（二次规划）**
> 考虑下面的二变元二次规划问题 $$\begin{align}\min\limits_{\boldsymbol{x}\in \mathbb{R}^{2}}~&~ \frac{1}{2}\begin{bmatrix}x_{1}\\x_{2}\end{bmatrix}^{\top}\begin{bmatrix}2&1\\1&4\end{bmatrix}\begin{bmatrix}x_{1}\\x_{2}\end{bmatrix} + \begin{bmatrix}5\\3\end{bmatrix}^{\top}\begin{bmatrix}x_{1}\\x_{2}\end{bmatrix}\tag{7.46}\\\text{subject to}~&~\begin{bmatrix}1&0\\-1&0\\0&1\\0&-1\end{bmatrix}\begin{bmatrix}x_{1}\\x_{2}\end{bmatrix} \leqslant \begin{bmatrix}1\\1\\1\\1\end{bmatrix}\tag{7.47}\end{align}$$ 
> 
> <center><img src="ch7/attachments/Pasted%20image%2020250701172628.png" alt="alt text" style="zoom:50%;"></center>
> 
> 由图可知，目标函数是二次的，矩阵 $\boldsymbol{Q}$ 是半正定的，因此我们看到的目标函数等高线是一系列椭圆。可行域是灰色区域，最优解由红色五角星表示。

二次规划的 Lagrangre 函数整理一下之后是
$$\begin{align}\mathfrak{L}(\boldsymbol{x}, \boldsymbol{\lambda}) &= \frac{1}{2}\boldsymbol{x}^{\top}\boldsymbol{Q}\boldsymbol{x} + \boldsymbol{c}^{\top}\boldsymbol{x} + \boldsymbol{\lambda}^{\top}(\boldsymbol{A}\boldsymbol{x} - \boldsymbol{b}) \tag{7.48a}\\&= \frac{1}{2}\boldsymbol{x}^{\top}\boldsymbol{Q}\boldsymbol{x} + (\boldsymbol{c} + \boldsymbol{A}^{\top}\boldsymbol{\lambda})^{\top}\boldsymbol{x} -\boldsymbol{\lambda}^{\top}\boldsymbol{b}, \tag{7.48b}\end{align}$$
求它对 $\boldsymbol{x}$ 的导数并令其为零，我们有
$$\boldsymbol{Q}\boldsymbol{x} + (\boldsymbol{c} + \boldsymbol{A}^{\top}\boldsymbol{\lambda}) = \boldsymbol{0}. \tag{7.49}$$
假设 $\boldsymbol{Q}$ 是可逆的，得到
$$\boldsymbol{x} = -\boldsymbol{Q}^{-1}(\boldsymbol{c} + \boldsymbol{A}^{\top}\boldsymbol{\lambda}). \tag{7.50}$$
把 (7.50) 代入最初的 Lagrangre 函数 $\mathfrak{L}(\boldsymbol{x} ,\boldsymbol{\lambda})$，我们得到 Lagrangre 对偶函数
$$\mathfrak{D}(\boldsymbol{\lambda}) = \frac{1}{2}(\boldsymbol{c} + \boldsymbol{A}^{\top}\boldsymbol{\lambda})^{\top}\boldsymbol{Q}^{-1}(\boldsymbol{c} + \boldsymbol{A}^{\top}\boldsymbol{\lambda}) - \boldsymbol{\lambda}^{\top}\boldsymbol{b}. \tag{7.51}$$
于是二次规划的对偶优化问题就是
$$\begin{align}\max\limits_{\boldsymbol{\lambda} \in \mathbb{R}^{m}}~&~ \frac{1}{2}(\boldsymbol{c} + \boldsymbol{A}^{\top}\boldsymbol{\lambda})^{\top}\boldsymbol{Q}{-1}(\boldsymbol{c} + \boldsymbol{A}^{\top}\boldsymbol{\lambda}) - \boldsymbol{\lambda}^{\top}\boldsymbol{b}\\\text{subject to}~&~ \boldsymbol{\lambda} \geqslant \boldsymbol{0}.\end{align} \tag{7.52}$$
我们将在第十二章的机器学习内容中再次见到二次规划。

### 7.3.3 Legendre-Fenchel 变换和凸共轭

让我们不考虑约束，重新回顾 7.2 节中的对偶概念。关于凸集的一个有用事实是，它可以用它的支撑超平面等价地描述。如果一个超平面与凸集相交，并且凸集只包含在它的一侧，则该超平面称为凸集的支撑超平面。回想一下，我们可以 “填充” 凸函数来获得上镜图，它是一个凸集。因此，我们也可以用它们的支撑超平面来描述凸函数。此外，观察到支撑超平面刚好与凸函数相切，实际上是该函数在该点的切线。回想一下，函数 $f (\boldsymbol{x})$ 在给定点 $\boldsymbol{x}_0$ 的切线是该函数在该点的梯度的求值 $\displaystyle \left. \frac{\mathrm{d}f(\boldsymbol{x})}{\mathrm{d}\boldsymbol{x}} \right|_{\boldsymbol{x} = \boldsymbol{x}_{0}}$ 。总而言之，由于凸集可以用其支撑超平面等效地描述，因此凸函数也可以用其梯度的函数等效地描述。**Legendre 变换**形式化地表达了这一概念。

> **注解**
> 物理系学生常常在学习经典力学中的 Lagrangre 量和 Hamilton 量的时候接触 Legendre 变换的

我们从最一般的定义开始，但它的形式有些违反直觉。我们先来看一些特殊情况，以便将定义与上一段描述的直觉联系起来。Legendre-Fenchel 变换是从凸可微函数 $f(\boldsymbol{x})$ 到依赖于切线 $s(\boldsymbol{x}) = \nabla_{\boldsymbol{x}}f(\boldsymbol{x})$ 的函数的变换（在傅里叶变换的意义上）。值得强调的是，这是函数 $f (\cdot)$ 的变换，而不是变量 $\boldsymbol{x}$ 或在 $\boldsymbol{x}$ 处求值的函数的变换。Legendre-Fenchel 变换也称为凸共轭（关于凸共轭的原因，我们很快就会看到），并且与对偶性密切相关（Hiriart-Urruty and Lemar´echal, 2001, 第五章）。

> **定义 7.4（凸共轭）**
> 函数 $f: \mathbb{R}^{D} \rightarrow \mathbb{R}$ 的 **凸共轭** 是$$f^{*}(\boldsymbol{s}) = \sup_{\boldsymbol{x} \in \mathbb{R}^{D}} \Big[ \left\langle \boldsymbol{s}, \boldsymbol{x} \right\rangle - f(\boldsymbol{x}) \Big] . \tag{7.53}$$

注意下文中提到的凸共轭并不需要函数 $f$ 是凸的或是可微的。定义 7.4 中，我们用的是抽象的内积记号（见 3.2），但下文中我们将继续使用有限维向量之间的标准内积（$\left\langle \boldsymbol{s}, \boldsymbol{x} \right\rangle = \boldsymbol{s}^{\top}\boldsymbol{x}$），以避免一些不必要的麻烦

> **注解**
> 画图能帮我们更好理解凸共轭的定义

为了从几何角度理解定义 7.4 的内容，考虑一个简单的一元可微的凸函数，例如 $f(x) = x^{2}$。注意我们考虑的是一元函数，超平面就是一条直线。考虑直线 $y = sx + c$ —— 我们可以用支撑超平面描述凸函数，因此让我们尝试用支撑超平面来描述函数 $f(x)$。固定直线的梯度 $s \in \mathbb{R}$ ，对于 $f$ 的图上的每个点 $(x_0, f(x_0))$，找到 $c$ 的最小值，使直线仍然经过 $(x_0, f(x_0))$ 相交。请注意，$c$ 的最小值是斜率为 $s$ 的直线刚好和函数 $f(x) = x^{2}$ 相切的位置。通过 $(x_0, f(x_0))$ 且梯度为 $s$ 的直线由  
$$y - f(x_{0}) = s(x - x_{0}). \tag{7.54}$$
给出。这条直线的 $y$ 轴截距为 $−sx_0 + f(x_0)$。因此，当 $y = sx + c$ 与 $f$ 的图像相交时，$c$ 的最小值为
$$\inf_{x_{0}} \Big[ -sx_{0} + f(x_{0}) \Big]. \tag{7.55} $$
按照惯例，前述凸共轭定义为其负值。本段的推理并不依赖于我们选择一维凸可微函数这一事实，并且对于 $f:\mathbb{R}^{D} \rightarrow \mathbb{R}$ 成立，它们是非凸且不可微的。

> **注解**
> 像 $f(x) = x^{2}$ 这样的可微凸函数是一个很好的特殊情况，我们不需要求上确界，且每个可微的凸函数和它的 Legendre 变换一一对应。让我们一步步导出这个结果。考虑可微凸函数 $f$，和 $(x_{0}, f(x_{0}))$ 处的切线 $$f(x_{0}) = sx_{0} + c. \tag{7.56}$$ 回忆可微凸函数 $f$ 和其梯度 $\nabla_{x}f(x)$ 的性质，我们有 $x = \nabla_{x}f(x_{0})$，整理上式，得到 $$-c = sx_{0} - f(x_{0}). \tag{7.57}$$ 注意 $c$ 随着 $x_{0}$（也即 $s$）的变化而变化，我们可以将其写为 $$f^{*}(s) \coloneqq sx_{0} - f(x_{0}). \tag{7.58}$$ 将 (7.58) 与定义 7.4 对比，容易发现前者是一个不带上确界的特殊情况。

凸共轭函数有不少良好的性质。例如对于凸函数，它的共轭的共轭是它本身。同样地，$f(x)$ 处的切线斜率是 $s$ 而 $f^{*}(s)$ 处的斜率是 $x$。下面的两个例子给出凸共轭在机器学习中的常见应用。

> **示例 7.7（凸共轭）**
> 为了展示凸共轭的应用，考虑下面的二次规划问题 $$f(\boldsymbol{y}) = \frac{\lambda}{2}\boldsymbol{y}^{\top}\boldsymbol{K}^{-1}\boldsymbol{y}\tag{7.59}$$其中 $\boldsymbol{K} \in \mathbb{R}^{n \times n}$ 是一个正定矩阵。我们定义主变量是 $\boldsymbol{y} \in \mathbb{R}^{n}$，对偶变量是 $\boldsymbol{\alpha} \in \mathbb{R}^{n}$。
> 根据定义 7.4，我们有 $$f^{*}(\boldsymbol{\alpha}) = \sup_{\boldsymbol{y}\in \mathbb{R}^{n}} \left[  \left\langle \boldsymbol{y}, \boldsymbol{\alpha} \right\rangle - \frac{\lambda}{2}\boldsymbol{y}^{\top}\boldsymbol{K}^{-1}\boldsymbol{y} \right]. \tag{7.60} $$由于该上确界中的函数是可微的，我们可以通过令其对 $\boldsymbol{y}$ 的梯度$$\displaystyle \frac{ \partial \left[  \left\langle \boldsymbol{y}, \boldsymbol{\alpha} \right\rangle - \frac{\lambda}{2}\boldsymbol{y}^{\top}\boldsymbol{K}^{-1}\boldsymbol{y} \right] }{ \partial \boldsymbol{y} } = (\boldsymbol{\alpha} - \lambda \boldsymbol{K}^{-1}\boldsymbol{y})^{\top}\tag{7.61}$$为零，也即当 $\displaystyle \boldsymbol{y} = \frac{1}{\lambda}\boldsymbol{K}\boldsymbol{\alpha}$，得到其最大值，也就是 $$f^{*}(\boldsymbol{\alpha}) = \frac{1}{\lambda}\boldsymbol{\alpha}^{\top}\boldsymbol{K}^{-1}\boldsymbol{\alpha} - \frac{\lambda}{2}\left( \frac{1}{\lambda}\boldsymbol{K}\boldsymbol{\alpha} \right)^{\top}\boldsymbol{K}^{-1}\left( \frac{1}{\lambda}\boldsymbol{K}\boldsymbol{\alpha} \right) = \frac{1}{2\lambda}\boldsymbol{\alpha}^{\top}\boldsymbol{K}\boldsymbol{\alpha}. \tag{7.62}$$

> **示例 7.8**
> 机器学习中，我们常用一系列函数（例如每条训练数据的损失函数 $\ell: \mathbb{R} \rightarrow \mathbb{R}$）的和作为目标。下面我们推导损失函数 $\ell(t)$ 之和的凸共轭，这同时展示了凸共轭在向量变元函数情况下的应用。令 $\displaystyle \mathcal{L}(\boldsymbol{t}) = \sum\limits_{i=1}^{n} \ell_{i}(t_{i})$，于是 $$\begin{align}\mathcal{L}^{*}(\boldsymbol{z}) &= \sup_{\boldsymbol{t} \in \mathbb{R}^{n}} \left[ \left\langle \boldsymbol{z}, \boldsymbol{t} \right\rangle -\sum\limits_{i=1}^{n} \ell_{i}(t_{i}) \right] \tag{7.63a}\\&= \sup_{\boldsymbol{t} \in \mathbb{R}^{n}} \sum\limits_{i=1}^{n}\left[ z_{i}t_{i} - \ell_{i}(t_{i})  \right] & \text{内积定义} \tag{7.63b}\\&= \sum\limits_{i=1}^{n} \sup_{\boldsymbol{t} \in \mathbb{R}^{n}} [z_{i}t_{i} - \ell_{i}(t_{i})] \tag{7.63c}\\&= \sum\limits_{i=1}^{n} \ell^{*}_{i}(z_{i}) & \text{共轭定义} \tag{7.63d}\end{align}$$

回忆在 7.2 节中，我们使用 Lagrangre 乘子导出原问题的对偶优化问题。进一步地，凸优化问题具有强对偶性：对偶问题的解就是原问题的解。本节中介绍的Legendre-Fenchel 变换也可以用来求对偶优化问题，特别地，当目标函数是可微且凸时，Legendre-Fenchel 变换中的上确界是唯一的。为了进一步说明这两个方法之间的联系，考虑下面带线性等式约束的凸优化问题。

> **示例 7.9**
> 考虑凸函数 $f(\boldsymbol{x})$，$g(\boldsymbol{x})$，实矩阵 $\boldsymbol{A}$，并假设方程 $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{y}$ 中的向量和矩阵形状匹配。于是
> $$\min\limits_{\boldsymbol{x}}~f(\boldsymbol{A}\boldsymbol{x}) + g(\boldsymbol{x}) = \min\limits_{\boldsymbol{A}\boldsymbol{x} = \boldsymbol{y}}~f(\boldsymbol{y}) + g(\boldsymbol{x}). \tag{7.64}$$ 引入约束 $\boldsymbol{A}\boldsymbol{x} = \boldsymbol{y}$ 和 Lagrangre 乘子 $\boldsymbol{u}$，有 $$\begin{align}\min\limits_{\boldsymbol{A}\boldsymbol{x} = \boldsymbol{y}}~f(\boldsymbol{y}) + g(\boldsymbol{x}) &=\min\limits_{\boldsymbol{x}, \boldsymbol{y}}~\max\limits_{\boldsymbol{u}}~f(\boldsymbol{y}) + g(\boldsymbol{x}) + (\boldsymbol{A}\boldsymbol{x} - \boldsymbol{y})^{\top}\boldsymbol{u} \tag{7.65a}\\&= \max\limits_{\boldsymbol{u}}~\min\limits_{\boldsymbol{x}, \boldsymbol{y}}~f(\boldsymbol{y}) + g(\boldsymbol{x}) + (\boldsymbol{A}\boldsymbol{x} -\boldsymbol{y})^{\top}\boldsymbol{u} \tag{7.65b}\end{align}$$ 其中最后一步可以交换 $\max$ 和 $\min$ 是因为 $f(\boldsymbol{y})$ 和 $g(\boldsymbol{x})$ 是凸函数。展开点积这一项，然后分开 $\boldsymbol{x}$ 和 $\boldsymbol{y}$ 的项，得到 $$\begin{align}&\max\limits_{\boldsymbol{u}}~\min\limits_{\boldsymbol{x}, \boldsymbol{y}}~f(\boldsymbol{y}) + g(\boldsymbol{x}) + (\boldsymbol{A}\boldsymbol{x} -\boldsymbol{y})^{\top}\boldsymbol{u} \tag{7.66a}\\=&\max\limits_{\boldsymbol{u}}~\Big[ \min\limits_{\boldsymbol{y}}~-\boldsymbol{y}^{\top}\boldsymbol{u} + f(\boldsymbol{y}) \Big] + \Big[ \min\limits_{\boldsymbol{x}}~(\boldsymbol{A}\boldsymbol{x})^{\top}\boldsymbol{u} + g(\boldsymbol{x}) \Big] \tag{7.66b}\\=& \max\limits_{\boldsymbol{u}}~\Big[ \min\limits_{\boldsymbol{y}}~-\boldsymbol{y}^{\top}\boldsymbol{u} + f(\boldsymbol{y}) \Big] + \Big[ \min\limits_{\boldsymbol{x}}~\boldsymbol{x}^{\top}\boldsymbol{A}^{\top}\boldsymbol{u} + g(\boldsymbol{x}) \Big] \tag{7.66c}\\\end{align}$$ 回忆凸共轭的定义（定义 7.4）以及（实）点积的对称性，我们有 $$\begin{align}&\max\limits_{\boldsymbol{u}}~\Big[ \min\limits_{\boldsymbol{y}}~-\boldsymbol{y}^{\top}\boldsymbol{u} + f(\boldsymbol{y}) \Big] + \Big[ \min\limits{\boldsymbol{x}}~\boldsymbol{x}^{\top}\boldsymbol{A}^{\top}\boldsymbol{u} + g(\boldsymbol{x}) \Big] \tag{7.67a}\\=& \max\limits_{\boldsymbol{u}}~-f^{*}(\boldsymbol{y}) - g^{*}(-\boldsymbol{A}^{\top}\boldsymbol{u}). \tag{7.67b}\end{align}$$ 于是我们就证明了 $$\min\limits_{\boldsymbol{x}}~f(\boldsymbol{A}\boldsymbol{x}) + g(\boldsymbol{x}) = \max\limits_{\boldsymbol{u}}~-f^{*}(\boldsymbol{u}) - g^{*}(-\boldsymbol{A}^{\top}\boldsymbol{u}). \tag{7.68}$$

事实上，Legendre-Fenchel 共轭在可表示为凸优化的机器学习中非常有用。特别地，对于独立作用于每个数据的损失函数，共轭损失函数是推导对偶问题的便捷方法。