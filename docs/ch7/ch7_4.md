## 7.4 拓展阅读
连续优化是一个活跃的研究领域，我们并不试图对近期进展进行全面的介绍。

从梯度下降的角度来看，它有两个主要弱点，每个弱点都有相应的文献。第一个挑战是梯度下降是一种一阶算法，它不使用有关表面曲率的信息。当存在长谷时，梯度垂直于感兴趣的方向。动量的概念可以推广到一类加速方法（Nesterov, 2018）。共轭梯度法通过考虑先前的方向来避免梯度下降面临的问题（Shewchuk, 1994）。二阶方法（如 Newton 法）使用 Hessian 矩阵来提供有关曲率的信息。许多选择步长和动量等想法的选择都是通过考虑目标函数的曲率而产生的（Goh, 2017; Bottou et al., 2018）。拟 Newton 法（如 L-BFGS）尝试使用更便宜的计算方法来近似 Hessian（Nocedal and Wright，2006）。最近，人们对计算下降方向的其他指标产生了兴趣，从而产生了诸如镜像下降（Beck and Teboulle，2003）和自然梯度（Toussaint, 2012）等方法。

第二个挑战是处理不可微函数。当函数中有扭结时，梯度方法定义不明确。在这些情况下，可以使用次梯度法（Shor, 1985）。有关优化不可微函数的更多信息和算法，请参阅 Bertsekas（1999）的书。有大量关于数值求解连续优化问题的不同方法的文献，包括约束优化问题的算法。理解这类文献的良好起点是阅读 Luenberger (1969) 和 Bonnans 等人 (2006) 的著作。Bubeck (2015) 提供了关于连续优化的最新综述。

现代机器学习的应用通常意味着数据集的大小限制了批量梯度下降的使用，因此随机梯度下降是当前大规模机器学习方法的主力。最近的文献综述包括 Hazan (2015) 和 Bottou et al. (2018)。

关于对偶和凸优化，Boyd and Vandenberghe (2004) 的著作包含在线讲座和幻灯片。Bertsekas (2009) 提供了更数学化的处理，而优化领域一位关键研究人员最近出版的著作是 Nesterov (2018)。凸优化基于凸分析，对凸函数更基础结果感兴趣的读者可以参考 Rockafellar (1970)、Hiriart-Urruty 和 Lemar´echal (2001) 以及 Borwein 和 Lewis (2006)。上述关于凸分析的书籍也涵盖了 Legendre–Fenchel 变换，但Zia等人（2009）的著作中提供了更适合初学者的介绍。Polyak（2016）的著作概述了Legendre–Fenchel 变换在凸优化算法分析中的作用。