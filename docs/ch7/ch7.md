# 第七章 连续优化

机器学习算法跑在计算机上，因此一切优化相关的数学设定需要被翻译为数值优化的方法。本章讲解了用于训练机器学习模型的简单数值方法。要训练一个机器学习模型，往往需要寻找一个最佳的参数集合，何谓 “最佳” 由目标函数或概率模型所确定（见本书的后半部分）。给定一个目标函数，我们会用优化算法找到它的最值（以及对应的最优参数集合）。

<center>
<img src="ch7/attachments/Pasted%20image%2020250625153238.png" alt="alt text" style="zoom:50%;">
</center>
<center>图 7.1 本章的概念地图</center>

本章包括连续优化中的两个主要分支——无约z束优化和约束优化，并总是假定目标函数是可微的（见第五章），这让我们能得到目标函数在空间中任意点出的梯度——这可以帮助我们找到最值点。一般地，大多数机器学习算法中我们要找对应目标函数的最小值（以及最小值点），所以直观的来说，我们要找目标函数的 “谷底”，而目标函数的梯度总是指向目标函数的 “高处”。所以我们的想法是逆着梯度的方向往 “下” 走，并祈祷我们能找到全局最小值点。对无约束优化而言，上述内容加上一些 *其他东西* 也就几乎是全部了（7.1 节）。但对于约束优化，我们要引进更多的概念来处理其中的 “约束”（7.2 节）。除此之外，我们在 7.3 节中还将引入一类特殊的优化问题（凸优化），它们有非常好的性质，可以以一些方式达到全局最优。

> > **注释**  
> 我们考虑的数据和模型工作在 $\mathbb{R}^{d}$ 上，我们处理的优化问题称为 **连续优化 (continuous optimization)** ；在另一边，也就是离散的世界，对应的优化问题称为 **组合优化 (combinatorial optimization)**。

<center>
<img src="ch7/attachments/Pasted%20image%2020250625151536.png" alt="alt text" style="zoom:50%;">
</center>
<center>图 7.2 一个目标函数示例。负梯度的方向用箭头表示，全局最小值点用蓝色虚线表示</center>

考虑图 7.2 中的函数，它在 $x = -4.5$ 附近有一个 **全局最小值点**，对应的函数值大概是 $-47$。这是一个 “光滑“ 的函数，我们可以用当前位置的梯度——它告诉我们应该向左走还是向右走——来找最小值。这样做其实假设我们处在一个盆地的结构：我们能看到函数在 $x = 0.7$ 附近有一个 **局部最小值点**。回忆一下，我们可以令导数为零得到函数的稳定点。

> > **注释**
> 稳定点是导函数的实根，对应所有梯度为零的点。

对于函数
$$
\ell(x) = x^{4} + 7x^{3} + 5x^{2} - 17x + 3, \tag{7.1}
$$
它对 $x$ 的导数是
$$
\displaystyle \frac{ \mathrm{d}\ell(x) }{ \mathrm{d}x } = 4x^{3} + 21x^{2} + 10x - 17\tag{7.2}
$$
这是个三次方程，一般有三个零点。在本例中，两个对应着局部最小值点，一个对应着局部最大值点。我们要再求一次导，来看一阶导的零点（也就是函数的稳定点）处的二阶导数值的符号：
$$
\displaystyle \frac{ \mathrm{d}^{2}\ell(x) }{ \mathrm{d}x^{2} } = 12x^{2} + 42x + 10 \tag{7.3}
$$
我们代入目测的三个极值点 $x = -4.5, -1.4, 0.7$，最终得到中间的那个极值点是一个局部最大值点 $\displaystyle \left( \frac{\mathrm{d}^{2}\ell(x)}{\mathrm{d}x^{2}} < 0 \right)$，而其他两个稳定点是局部最小值点。

读者也许发现我们在上面的例子中刻意避免了求导数等于零这个方程的解析解——虽然对于阶数较低多项式我们可以这么做——但在一般情况下我们几乎不可能接触目标函数一阶导为零的解析解。因此我们转向数值解法：找一个初始点，例如 $x_{0} = -6$，然后跟着负梯度的方向走。在图中，负梯度的箭头告诉我们我们应该向右走，但我们不知道应该向右走多远（这叫做 **步长**）。进一步地，如果我们的初始值取在右边（例如 $x_{0} = 0$），负梯度方向将把我们带到一个 ”错误的“ 最小值点。我们在图 7.2 中可以看到，右侧的负梯度方向指向的是局部最优解，它对应的函数值大于函数的最小值（左侧）。

> > **注释**
> 根据 Abel-Ruffni 定理，五次及以上的多项式方程没有代数解。(Abel, 1826)

在 7.3 节中，我们将会见到一族称为 ”凸函数“ 的函数，初始点的选取不会对优化算法的结果造成像前文那样的影响。**对于凸函数来说，它的局部最小值点总是全局最小值点。** 事实上，很多机器学习算法的目标函数都被设计为凸函数，我们将会在第十二章见到一个例子。

到此为止，本章的讨论内容仅仅局限于一元函数，这是为了更方便地展示梯度、下降方向和最优值。在接下来的内容，我们将在高位空间中使用类似的想法。不幸的是，一些概念不能简单推广到高维空间，所以在遇到似曾相识的概念时需要多加小心。


