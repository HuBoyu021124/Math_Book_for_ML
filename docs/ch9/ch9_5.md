## 9.5 进一步阅读

在本章中，我们讨论了具有高斯似然和共轭高斯先验的线性回归模型的参数。这使得我们可以进行闭式贝叶斯推断。然而，在某些应用中，我们可能希望选择不同的似然函数。例如，在二分类设置中，我们观察到的只有两个可能的（分类）结果，高斯似然在这种情况下是不合适的。相反，我们可以选择伯努利似然，它将返回预测标签为1（或0）的概率。我们推荐Barber（2012）、Bishop（2006）和Murphy（2012）的书籍，以深入了解分类问题。

另一个例子是计数数据。计数是非负整数，在这种情况下，二项分布或泊松分布比高斯分布是更好的选择。所有这些例子都属于广义线性模型（Generalized Linear Models, GLM）的范畴，这是线性回归的一种灵活推广，允许响应变量具有高斯分布之外的误差分布。广义线性模型通过一个平滑且可逆的函数  $ \sigma(\cdot) $  将线性模型与观测值联系起来，该函数可能是非线性的，使得  $ y = \sigma(f(x)) $ ，其中  $ f(x) = \theta^\top \phi(x) $  是式(9.13)中的线性回归模型。因此，我们可以将广义线性模型视为函数复合  $ y = \sigma \circ f $ ，其中  $ f $  是线性回归模型， $ \sigma $  是激活函数。需要注意的是，尽管我们在这里讨论的是“广义线性模型”，但输出  $ y $  不再是参数  $ \theta $  的线性函数。在逻辑回归中，我们选择逻辑函数  $ \sigma(f) = \frac{1}{1 + \exp(-f)} \in [0, 1] $ ，它可以被解释为伯努利随机变量  $ y \in \{0, 1\} $  观测值为1的概率。函数  $ \sigma(\cdot) $  被称为转移函数或激活函数，其逆被称为典型链接函数。从这个角度来看，普通线性回归的激活函数仅仅是恒等函数。

此外，很明显广义线性模型是（深度）前馈神经网络的构建块：如果我们考虑一个广义线性模型  $ y = \sigma(Ax + b) $ ，其中  $ A $  是权重矩阵， $ b $  是偏置向量，我们将这个广义线性模型识别为一个具有激活函数  $ \sigma(\cdot) $  的单层神经网络。我们可以通过以下方式递归组合这些函数：

 $$
x_{k+1} = f_k(x_k) \quad f_k(x_k) = \sigma_k(A_k x_k + b_k) \tag{9.72}
$$

其中  $ k = 0, \ldots, K-1 $ ， $ x_0 $  是输入特征， $ x_K = y $  是观测输出，使得  $ f_{K-1} \circ \cdots \circ f_0 $  是一个  $ K $  层深度神经网络。因此，这个深度神经网络的构建块是式(9.72)中定义的广义线性模型。关于GLM和深度网络之间关系的详细介绍，可以参考 [https://tinyurl.com/glm-dnn](https://tinyurl.com/glm-dnn)。

神经网络（Bishop, 1995; Goodfellow et al., 2016）比线性回归模型具有更高的表达能力和灵活性。然而，最大似然参数估计是一个非凸优化问题，而在完全贝叶斯设置中对参数进行边缘化是解析上不可行的。我们在前面的内容中简要提到，一个参数分布可以诱导一个函数分布。高斯过程（Gaussian Process, GP）是一种回归模型，其中函数分布的概念是核心。高斯过程通过利用核技巧（Schölkopf and Smola, 2002），直接在函数空间上放置分布，而无需通过参数的“绕道”。核技巧允许我们通过查看对应的输入  $ x_i, x_j $  来计算两个函数值  $ f(x_i), f(x_j) $  之间的内积。高斯过程与贝叶斯线性回归和支撑向量回归密切相关，也可以被解释为贝叶斯神经网络的一个单隐藏层版本，其中隐藏单元的数量趋于无穷大（Neal, 1996; Williams, 1997）。关于高斯过程的优秀介绍可以参考MacKay（1998）和Rasmussen and Williams（2006）。

我们在本章的讨论中专注于高斯参数先验，因为它们允许我们在线性回归模型中进行闭式推理。然而，即使在具有高斯似然的回归设置中，我们也可以选择非高斯先验。考虑一个输入  $ x \in \mathbb{R}^D $  的设置，我们的训练集很小，大小为  $ N \ll D $ 。这意味着回归问题是欠定的。在这种情况下，我们可以选择一个强制稀疏性的参数先验，即一个试图将尽可能多的参数设置为0的先验（变量选择）。这个先验提供了一个比高斯先验更强的正则化器，通常可以提高模型的预测精度和可解释性。拉普拉斯先验是一个经常用于此目的的例子。具有拉普拉斯先验的线性回归模型等同于具有L1正则化的线性回归（LASSO）（Tibshirani, 1996）。拉普拉斯分布的峰值
在零处非常尖锐（其一阶导数不连续），并且它将概率质量更集中在零附近，这比高斯分布更倾向于使参数为0。因此，非零参数对于回归问题是相关的，这也是我们所说的“变量选择”的原因。
